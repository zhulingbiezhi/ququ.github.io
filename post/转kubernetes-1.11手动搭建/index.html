<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>【转】kubernetes 1.11手动搭建 - 去去的blog - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="去去" /><meta name="description" content="kubernetes 1.11手动搭建" />
<meta name="keywords" content="kubernetes" />







<meta name="generator" content="Hugo 0.46" />


<link rel="canonical" href="http://zhulingbiezhi.github.io/post/%E8%BD%ACkubernetes-1.11%E6%89%8B%E5%8A%A8%E6%90%AD%E5%BB%BA/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">







<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="【转】kubernetes 1.11手动搭建" />
<meta property="og:description" content="kubernetes 1.11手动搭建" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://zhulingbiezhi.github.io/post/%E8%BD%ACkubernetes-1.11%E6%89%8B%E5%8A%A8%E6%90%AD%E5%BB%BA/" />



<meta property="article:published_time" content="2018-08-22T15:16:48&#43;08:00"/>

<meta property="article:modified_time" content="2018-08-22T15:16:48&#43;08:00"/>











<meta itemprop="name" content="【转】kubernetes 1.11手动搭建">
<meta itemprop="description" content="kubernetes 1.11手动搭建">


<meta itemprop="datePublished" content="2018-08-22T15:16:48&#43;08:00" />
<meta itemprop="dateModified" content="2018-08-22T15:16:48&#43;08:00" />
<meta itemprop="wordCount" content="11209">



<meta itemprop="keywords" content="kubernetes,分布式," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="【转】kubernetes 1.11手动搭建"/>
<meta name="twitter:description" content="kubernetes 1.11手动搭建"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">去去</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">类别</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">去去</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">类别</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">【转】kubernetes 1.11手动搭建</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-22 </span>
        <div class="post-category">
            
              <a href="/categories/kubernetes/"> kubernetes </a>
            
          </div>
        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#kubernetes部署信息">Kubernetes部署信息</a></li>
<li><a href="#节点信息">节点信息</a></li>
<li><a href="#事前准备">事前准备</a></li>
</ul></li>
<li><a href="#不同文件会有差异">不同文件会有差异</a></li>
<li><a href="#node-可忽略下載-kubectl">Node 可忽略下載 kubectl</a>
<ul>
<li><a href="#建立ca与产生tls凭证">建立CA与产生TLS凭证</a>
<ul>
<li><a href="#etcd">ETCD</a></li>
<li><a href="#kubernetes组件">Kubernetes组件</a>
<ul>
<li><a href="#api-server">API Server</a></li>
<li><a href="#front-proxy-client">Front Proxy Client</a></li>
<li><a href="#controller-manager">Controller Manager</a></li>
<li><a href="#scheduler">Scheduler</a></li>
<li><a href="#admin">Admin</a></li>
<li><a href="#masters-kubelet">Masters Kubelet</a></li>
<li><a href="#service-account-key">Service Account Key</a></li>
<li><a href="#删除不必要文件">删除不必要文件</a></li>
<li><a href="#复制文件至其他节点">复制文件至其他节点</a></li>
</ul></li>
</ul></li>
<li><a href="#kubernetes-masters">Kubernetes Masters</a>
<ul>
<li><a href="#部署与设定">部署与设定</a></li>
<li><a href="#建立-tls-bootstrapping">建立 TLS Bootstrapping</a></li>
<li><a href="#验证master节点">验证Master节点</a></li>
</ul></li>
<li><a href="#kubernetes-nodes">Kubernetes Nodes</a>
<ul>
<li><a href="#部署与设定-1">部署与设定</a></li>
<li><a href="#验证node节点">验证Node节点</a></li>
</ul></li>
<li><a href="#kubernetes-core-addons部署">Kubernetes Core Addons部署</a>
<ul>
<li><a href="#kubernetes-proxy">Kubernetes Proxy</a></li>
</ul></li>
</ul></li>
<li><a href="#检查-log-是否使用-ipvs">检查 log 是否使用 ipvs</a>
<ul>
<li>
<ul>
<li><a href="#coredns">CoreDNS</a></li>
</ul></li>
<li><a href="#kubernetes集群网路">Kubernetes集群网路</a>
<ul>
<li><a href="#网路部署与设定">网路部署与设定</a></li>
</ul></li>
<li><a href="#kubernetes-extra-addons部署">Kubernetes Extra Addons部署</a>
<ul>
<li><a href="#ingress-controller">Ingress Controller</a></li>
</ul></li>
</ul></li>
<li><a href="#测试其他-domain-name-是是否回应-404">测试其他 domain name 是是否回应 404</a>
<ul>
<li>
<ul>
<li><a href="#external-dns">External DNS</a></li>
<li><a href="#dashboard">Dashboard</a></li>
<li><a href="#prometheus">Prometheus</a></li>
<li><a href="#metrics-server">Metrics Server</a></li>
<li><a href="#helm-tiller-server">Helm Tiller Server</a>
<ul>
<li><a href="#测试helm功能">测试Helm功能</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#取得-admin-账号的密码">取得 admin 账号的密码</a>
<ul>
<li><a href="#测试集群ha功能">测试集群HA功能</a></li>
</ul></li>
<li><a href="#先检查-etcd-状态-可以发现-etcd-0-因为关机而中断">先检查 etcd 状态，<em>可以发现 etcd-0 因为关机而中断</em></a></li>
<li><a href="#测试是否可以建立-pod">测试是否可以建立 Pod</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>本篇延续过往手动安装方式来部署 <a href="https://www.kubernetes.org.cn/tags/kubernetes1-11">Kubernetes v1.11.x</a> 版本的 High Availability 集群，而此次教学将直接透过裸机进行部署 Kubernetes 集群。以手动安装的目标是学习 Kubernetes 各组件关析、流程、设定与部署方式。若不想这么累的话，可以参考 <a href="https://kubernetes.io/docs/getting-started-guides/">Picking the Right Solution</a> 来选择自己最喜欢的方式。</p>

<p><img src="https://kairen.github.io/images/kube/kubernetes-aa-ha.png" alt="" /></p>

<h2 id="kubernetes部署信息">Kubernetes部署信息</h2>

<p>Kubernetes部署的版本信息：</p>

<ul>
<li>Kubernetes: v1.11.0</li>
<li>CNI: v0.7.1</li>
<li>Etcd: v3.3.8</li>
<li>Docker: v18.05.0-ce</li>
<li>Calico: v3.1</li>
</ul>

<p>Kubernetes部署的网路信息：</p>

<ul>
<li>Cluster IP CIDR: 10.244.0.0/16</li>
<li>Service Cluster IP CIDR: 10.96.0.0/12</li>
<li>Service DNS IP: 10.96.0.10</li>
<li>DNS DN: cluster.local</li>
<li>Kubernetes API VIP: 172.22.132.9</li>
<li>Kubernetes Ingress VIP: 172.22.132.8</li>
</ul>

<h2 id="节点信息">节点信息</h2>

<p>本教程采用以下节点数与机器规格进行部署裸机(Bare-metal)，操作系统采用Ubuntu 16+(理论上 CentOS 7+ 也行)进行测试：</p>

<table>
<thead>
<tr>
<th align="center">IP Address</th>
<th align="center">Hostname</th>
<th align="center">CPU</th>
<th align="center">Memory</th>
<th align="center">Extra Device</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">172.22.132.10</td>
<td align="center">K8S-M1</td>
<td align="center">4</td>
<td align="center">16G</td>
<td align="center">None</td>
</tr>

<tr>
<td align="center">172.22.132.11</td>
<td align="center">K8S-M2</td>
<td align="center">4</td>
<td align="center">16G</td>
<td align="center">None</td>
</tr>

<tr>
<td align="center">172.22.132.12</td>
<td align="center">K8S-M3</td>
<td align="center">4</td>
<td align="center">16G</td>
<td align="center">None</td>
</tr>
</tbody>
</table>

<p>另外由所有 master 节点提供一组 VIP 172.22.132.9。</p>

<ul>
<li>这边m为 K8s Master 节点，g为 K8s Node 节点。</li>
<li>所有操作全部用root使用者进行，主要方便部署用。</li>
</ul>

<h2 id="事前准备">事前准备</h2>

<p>开始部署集群前需先确保以下条件已达成：</p>

<ul>
<li>所有节点彼此网络互通，并且k8s-m1 SSH 登入其他节点为 passwdless，由于过程中很多会在某台节点(k8s-m1)上以 SSH 复制与操作其他节点。</li>
<li>确认所有防火墙与 SELinux 已关闭。如 CentOS：</li>
</ul>

<p>$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld
$ setenforce 0
$ vim /etc/selinux/config
SELINUX=disabled</p>

<blockquote>
<p>关闭是为了方便安装使用，若有需要防火墙可以参考 <a href="https://kubernetes.io/docs/tasks/tools/install-kubeadm/#check-required-ports">Required ports</a> 来设定。</p>

<ul>
<li>所有节点需要设定/etc/hosts解析到所有集群主机。</li>
</ul>
</blockquote>

<p>&hellip;
172.22.132.10 k8s-m1
172.22.132.11 k8s-m2
172.22.132.12 k8s-m3
172.22.132.13 k8s-g1
172.22.132.14 k8s-g2</p>

<ul>
<li>所有节点需要安装 Docker CE 版本的容器引擎：</li>
</ul>

<p>$ curl -fsSL <a href="https://get.docker.com/">https://get.docker.com/</a> | sh</p>

<blockquote>
<p>不管是在 Ubuntu 或 CentOS 都只需要执行该指令就会自动安装最新版 Docker。<br />
CentOS 安装完成后，需要再执行以下指令：</p>

<p>$ systemctl enable docker &amp;&amp; systemctl start docker</p>
</blockquote>

<ul>
<li>所有节点需要设定以下系统参数。</li>
</ul>

<p>$ cat &lt;&lt;EOF | tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF</p>

<p>$ sysctl -p /etc/sysctl.d/k8s.conf</p>

<blockquote>
<p>关于bridge-nf-call-iptables的启用取决于是否将容器连接到Linux bridge或使用其他一些机制(如 SDN vSwitch)。</p>
</blockquote>

<ul>
<li>Kubernetes v1.8+ 要求关闭系统 Swap，请在所有节点利用以下指令关闭：</li>
</ul>

<p>$ swapoff -a &amp;&amp; sysctl -w vm.swappiness=0</p>

<h1 id="不同文件会有差异">不同文件会有差异</h1>

<p>$ sed &lsquo;/swap.img/d&rsquo; -i /etc/fstab</p>

<blockquote>
<p>记得/etc/fstab也要批注掉SWAP挂载。</p>
</blockquote>

<ul>
<li>在所有节点下载Kubernetes二进制执行文件：</li>
</ul>

<p>$ export KUBE_URL=<a href="https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/linux/amd64">https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/linux/amd64</a>
$ wget ${KUBE_URL}/kubelet -O /usr/local/bin/kubelet
$ chmod +x /usr/local/bin/kubelet</p>

<h1 id="node-可忽略下載-kubectl">Node 可忽略下載 kubectl</h1>

<p>$ wget ${KUBE_URL}/kubectl -O /usr/local/bin/kubectl
$ chmod +x /usr/local/bin/kubectl</p>

<ul>
<li>在所有节点下载Kubernetes CNI二进制执行文件：</li>
</ul>

<p>$ export CNI_URL=<a href="https://github.com/containernetworking/plugins/releases/download">https://github.com/containernetworking/plugins/releases/download</a>
$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin
$ wget -qO- &ndash;show-progress &ldquo;${CNI_URL}/v0.7.1/cni-plugins-amd64-v0.7.1.tgz&rdquo; | tar -zx</p>

<ul>
<li>在k8s-m1节点安装cfssl工具，这将会用来建立CA，并产生TLS凭证。</li>
</ul>

<p>$ export CFSSL_URL=<a href="https://pkg.cfssl.org/R1.2">https://pkg.cfssl.org/R1.2</a>
$ wget ${CFSSL_URL}/cfssl_linux-amd64 -O /usr/local/bin/cfssl
$ wget ${CFSSL_URL}/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson
$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson</p>

<h2 id="建立ca与产生tls凭证">建立CA与产生TLS凭证</h2>

<p>本节将会通过 CFSSL 工具来产生不同组件的凭证，如 Etcd、Kubernetes API Server 等等，其中各组件都会有一个根数字证书认证机构(Root Certificate Authority)被用在组件之间的认证。</p>

<blockquote>
<p>要注意 CA JSON 檔中的CN(Common Name)与O(Organization)等内容是会影响 Kubernetes 组件认证的。</p>
</blockquote>

<p>首先在k8s-m1通过 Git 取得部署用文件：</p>

<p>$ git clone <a href="https://github.com/kairen/k8s-manual-files.git">https://github.com/kairen/k8s-manual-files.git</a> ~/k8s-manual-files
$ cd ~/k8s-manual-files/pki</p>

<h3 id="etcd">ETCD</h3>

<p>在k8s-m1建立/etc/etcd/ssl文件夹，并产生 Etcd CA：</p>

<p>$ export DIR=/etc/etcd/ssl
$ mkdir -p ${DIR}
$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare ${DIR}/etcd-ca</p>

<p>接着产生Etcd凭证：</p>

<p>$ cfssl gencert
  -ca=${DIR}/etcd-ca.pem
  -ca-key=${DIR}/etcd-ca-key.pem
  -config=ca-config.json
  -hostname=127.0.0.1,172.22.132.10,172.22.132.11,172.22.132.12
  -profile=kubernetes
  etcd-csr.json | cfssljson -bare ${DIR}/etcd</p>

<blockquote>
<p>-hostname需修改成所有masters节点。</p>
</blockquote>

<p>删除不必要的文件，检查并/etc/etcd/ssl目录是否成功建立以下文件：</p>

<p>$ rm -rf ${DIR}/*.csr
$ ls /etc/etcd/ssl
etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem</p>

<p>复制文件至其他Etcd节点，这边为所有master节点：</p>

<p>$ for NODE in k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    ssh ${NODE} &ldquo; mkdir -p /etc/etcd/ssl&rdquo;
    for FILE in etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem; do
      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}
    done
  done</p>

<h3 id="kubernetes组件">Kubernetes组件</h3>

<p>在k8s-m1建立/etc/kubernetes/pki，并依据下面指令来产生CA：</p>

<p>$ export K8S_DIR=/etc/kubernetes
$ export PKI_DIR=${K8S_DIR}/pki
$ export KUBE_APISERVER=<a href="https://172.22.132.9:6443">https://172.22.132.9:6443</a>
$ mkdir -p ${PKI_DIR}
$ cfssl gencert -initca ca-csr.json | cfssljson -bare ${PKI_DIR}/ca
$ ls ${PKI_DIR}/ca*.pem
/etc/kubernetes/pki/ca-key.pem  /etc/kubernetes/pki/ca.pem</p>

<blockquote>
<p>KUBE_APISERVER这边设定为VIP位址。</p>
</blockquote>

<p>接着依照以下小节来建立TLS凭证。</p>

<h4 id="api-server">API Server</h4>

<p>通过以下指令产生Kubernetes API Server凭证：</p>

<p>$ cfssl gencert
  -ca=${PKI_DIR}/ca.pem
  -ca-key=${PKI_DIR}/ca-key.pem
  -config=ca-config.json
  -hostname=10.96.0.1,172.22.132.9,127.0.0.1,kubernetes.default
  -profile=kubernetes
  apiserver-csr.json | cfssljson -bare ${PKI_DIR}/apiserver</p>

<p>$ ls ${PKI_DIR}/apiserver*.pem
/etc/kubernetes/pki/apiserver-key.pem  /etc/kubernetes/pki/apiserver.pem</p>

<blockquote>
<p>这边-hostname的10.96.0.1是 Cluster IP 的 Kubernetes 端点; 172.22.132.9为 VIP 地址; kubernetes.default为 Kubernetes 系统在 default namespace 自动建立的 API service domain name。</p>
</blockquote>

<h4 id="front-proxy-client">Front Proxy Client</h4>

<p>此凭证将被用于 Authenticating Proxy 的功能上，而该功能主要是提供 API Aggregation 的认证。首先通过以下指令产生 CA：</p>

<p>$ cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-ca
$ ls ${PKI_DIR}/front-proxy-ca*.pem
/etc/kubernetes/pki/front-proxy-ca-key.pem  /etc/kubernetes/pki/front-proxy-ca.pem</p>

<p>接着产生 Front proxy client 凭证：</p>

<p>$ cfssl gencert
  -ca=${PKI_DIR}/front-proxy-ca.pem
  -ca-key=${PKI_DIR}/front-proxy-ca-key.pem
  -config=ca-config.json
  -profile=kubernetes
  front-proxy-client-csr.json | cfssljson -bare ${PKI_DIR}/front-proxy-client</p>

<p>$ ls ${PKI_DIR}/front-proxy-client*.pem
/etc/kubernetes/pki/front-proxy-client-key.pem  /etc/kubernetes/pki/front-proxy-client.pem</p>

<h4 id="controller-manager">Controller Manager</h4>

<p>凭证会建立system:kube-controller-manager的使用者(凭证 CN)，并被绑定在 RBAC Cluster Role 中的system:kube-controller-manager来让 Controller Manager 组件能够存取需要的 API object。这边通过以下指令产生 Controller Manager 凭证：</p>

<p>$ cfssl gencert
  -ca=${PKI_DIR}/ca.pem
  -ca-key=${PKI_DIR}/ca-key.pem
  -config=ca-config.json
  -profile=kubernetes
  manager-csr.json | cfssljson -bare ${PKI_DIR}/controller-manager</p>

<p>$ ls ${PKI_DIR}/controller-manager*.pem
/etc/kubernetes/pki/controller-manager-key.pem  /etc/kubernetes/pki/controller-manager.pem</p>

<p>接着利用kubectl来产生Controller Manager的kubeconfig档：</p>

<p>$ kubectl config set-cluster kubernetes
    &ndash;certificate-authority=${PKI_DIR}/ca.pem
    &ndash;embed-certs=true
    &ndash;server=${KUBE_APISERVER}
    &ndash;kubeconfig=${K8S_DIR}/controller-manager.conf</p>

<p>$ kubectl config set-credentials system:kube-controller-manager
    &ndash;client-certificate=${PKI_DIR}/controller-manager.pem
    &ndash;client-key=${PKI_DIR}/controller-manager-key.pem
    &ndash;embed-certs=true
    &ndash;kubeconfig=${K8S_DIR}/controller-manager.conf</p>

<p>$ kubectl config set-context system:kube-controller-manager@kubernetes
    &ndash;cluster=kubernetes
    &ndash;user=system:kube-controller-manager
    &ndash;kubeconfig=${K8S_DIR}/controller-manager.conf</p>

<p>$ kubectl config use-context system:kube-controller-manager@kubernetes
    &ndash;kubeconfig=${K8S_DIR}/controller-manager.conf</p>

<h4 id="scheduler">Scheduler</h4>

<p>凭证会建立system:kube-scheduler的使用者(凭证 CN)，并被绑定在 RBAC Cluster Role 中的system:kube-scheduler来让 Scheduler 组件能够存取需要的 API object。这边通过以下指令产生 Scheduler 凭证：</p>

<p>$ cfssl gencert
  -ca=${PKI_DIR}/ca.pem
  -ca-key=${PKI_DIR}/ca-key.pem
  -config=ca-config.json
  -profile=kubernetes
  scheduler-csr.json | cfssljson -bare ${PKI_DIR}/scheduler</p>

<p>$ ls ${PKI_DIR}/scheduler*.pem
/etc/kubernetes/pki/scheduler-key.pem  /etc/kubernetes/pki/scheduler.pem</p>

<p>接着利用kubectl来产生Scheduler的kubeconfig文件：</p>

<p>$ kubectl config set-cluster kubernetes
    &ndash;certificate-authority=${PKI_DIR}/ca.pem
    &ndash;embed-certs=true
    &ndash;server=${KUBE_APISERVER}
    &ndash;kubeconfig=${K8S_DIR}/scheduler.conf</p>

<p>$ kubectl config set-credentials system:kube-scheduler
    &ndash;client-certificate=${PKI_DIR}/scheduler.pem
    &ndash;client-key=${PKI_DIR}/scheduler-key.pem
    &ndash;embed-certs=true
    &ndash;kubeconfig=${K8S_DIR}/scheduler.conf</p>

<p>$ kubectl config set-context system:kube-scheduler@kubernetes
    &ndash;cluster=kubernetes
    &ndash;user=system:kube-scheduler
    &ndash;kubeconfig=${K8S_DIR}/scheduler.conf</p>

<p>$ kubectl config use-context system:kube-scheduler@kubernetes
    &ndash;kubeconfig=${K8S_DIR}/scheduler.conf</p>

<h4 id="admin">Admin</h4>

<p>Admin 被用来绑定 RBAC Cluster Role 中 cluster-admin，当想要操作所有 Kubernetes 集群功能时，就必须利用这边产生的 kubeconfig 文件案。这边通过以下指令产生 Kubernetes Admin 凭证：</p>

<p>$ cfssl gencert
  -ca=${PKI_DIR}/ca.pem
  -ca-key=${PKI_DIR}/ca-key.pem
  -config=ca-config.json
  -profile=kubernetes
  admin-csr.json | cfssljson -bare ${PKI_DIR}/admin</p>

<p>$ ls ${PKI_DIR}/admin*.pem
/etc/kubernetes/pki/admin-key.pem  /etc/kubernetes/pki/admin.pem</p>

<p>接着利用kubectl来产生Admin的kubeconfig文件：</p>

<p>$ kubectl config set-cluster kubernetes
    &ndash;certificate-authority=${PKI_DIR}/ca.pem
    &ndash;embed-certs=true
    &ndash;server=${KUBE_APISERVER}
    &ndash;kubeconfig=${K8S_DIR}/admin.conf</p>

<p>$ kubectl config set-credentials kubernetes-admin
    &ndash;client-certificate=${PKI_DIR}/admin.pem
    &ndash;client-key=${PKI_DIR}/admin-key.pem
    &ndash;embed-certs=true
    &ndash;kubeconfig=${K8S_DIR}/admin.conf</p>

<p>$ kubectl config set-context kubernetes-admin@kubernetes
    &ndash;cluster=kubernetes
    &ndash;user=kubernetes-admin
    &ndash;kubeconfig=${K8S_DIR}/admin.conf</p>

<p>$ kubectl config use-context kubernetes-admin@kubernetes
    &ndash;kubeconfig=${K8S_DIR}/admin.conf</p>

<h4 id="masters-kubelet">Masters Kubelet</h4>

<p>这边使用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node authorizer</a> 来让节点的 kubelet 能够存取如 services、endpoints 等 API，而使用 Node authorizer 需定义 system:nodes 群组(凭证的 Organization)，并且包含system:node:<nodeName>的使用者名称(凭证的 Common Name)。</p>

<p>首先在k8s-m1节点产生所有 master 节点的 kubelet 凭证，这边通过下面脚本来产生：</p>

<p>$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    cp kubelet-csr.json kubelet-$NODE-csr.json;
    sed -i &ldquo;s/$NODE/$NODE/g&rdquo; kubelet-$NODE-csr.json;
    cfssl gencert
      -ca=${PKI_DIR}/ca.pem
      -ca-key=${PKI_DIR}/ca-key.pem
      -config=ca-config.json
      -hostname=$NODE
      -profile=kubernetes
      kubelet-$NODE-csr.json | cfssljson -bare ${PKI_DIR}/kubelet-$NODE;
    rm kubelet-$NODE-csr.json
  done</p>

<p>$ ls ${PKI_DIR}/kubelet*.pem
/etc/kubernetes/pki/kubelet-k8s-m1-key.pem  /etc/kubernetes/pki/kubelet-k8s-m2.pem
/etc/kubernetes/pki/kubelet-k8s-m1.pem      /etc/kubernetes/pki/kubelet-k8s-m3-key.pem
/etc/kubernetes/pki/kubelet-k8s-m2-key.pem  /etc/kubernetes/pki/kubelet-k8s-m3.pem</p>

<p>产生完成后，将kubelet凭证复制到所有master节点上：</p>

<p>$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    ssh ${NODE} &ldquo;mkdir -p ${PKI_DIR}&rdquo;
    scp ${PKI_DIR}/ca.pem ${NODE}:${PKI_DIR}/ca.pem
    scp ${PKI_DIR}/kubelet-$NODE-key.pem ${NODE}:${PKI_DIR}/kubelet-key.pem
    scp ${PKI_DIR}/kubelet-$NODE.pem ${NODE}:${PKI_DIR}/kubelet.pem
    rm ${PKI_DIR}/kubelet-$NODE-key.pem ${PKI_DIR}/kubelet-$NODE.pem
  done</p>

<p>接着利用 kubectl 来产生 kubelet 的 kubeconfig 文件，这边通过脚本来产生所有master节点的文件：</p>

<p>$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    ssh ${NODE} &ldquo;cd ${PKI_DIR} &amp;&amp;
      kubectl config set-cluster kubernetes
        &ndash;certificate-authority=${PKI_DIR}/ca.pem
        &ndash;embed-certs=true
        &ndash;server=${KUBE_APISERVER}
        &ndash;kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp;
      kubectl config set-credentials system:node:${NODE}
        &ndash;client-certificate=${PKI_DIR}/kubelet.pem
        &ndash;client-key=${PKI_DIR}/kubelet-key.pem
        &ndash;embed-certs=true
        &ndash;kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp;
      kubectl config set-context system:node:${NODE}@kubernetes
        &ndash;cluster=kubernetes
        &ndash;user=system:node:${NODE}
        &ndash;kubeconfig=${K8S_DIR}/kubelet.conf &amp;&amp;
      kubectl config use-context system:node:${NODE}@kubernetes
        &ndash;kubeconfig=${K8S_DIR}/kubelet.conf&rdquo;
  done</p>

<h4 id="service-account-key">Service Account Key</h4>

<p>Kubernetes Controller Manager 利用 Key pair 来产生与签署 Service Account 的 tokens，而这边不通过 CA 做认证，而是建立一组公私钥来让 API Server 与 Controller Manager 使用：</p>

<p>$ openssl genrsa -out ${PKI_DIR}/sa.key 2048
$ openssl rsa -in ${PKI_DIR}/sa.key -pubout -out ${PKI_DIR}/sa.pub
$ ls ${PKI_DIR}/sa.*
/etc/kubernetes/pki/sa.key  /etc/kubernetes/pki/sa.pub</p>

<h4 id="删除不必要文件">删除不必要文件</h4>

<p>当所有文件建立与产生完成后，将一些不必要文件删除：</p>

<p>$ rm -rf ${PKI_DIR}/<em>.csr
    ${PKI_DIR}/scheduler</em>.pem
    ${PKI_DIR}/controller-manager<em>.pem
    ${PKI_DIR}/admin</em>.pem
    ${PKI_DIR}/kubelet*.pem</p>

<h4 id="复制文件至其他节点">复制文件至其他节点</h4>

<p>凭证将复制到其他master节点：</p>

<p>$ for NODE in k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    for FILE in $(ls ${PKI_DIR}); do
      scp ${PKI_DIR}/${FILE} ${NODE}:${PKI_DIR}/${FILE}
    done
  done</p>

<p>复制kubeconfig文件至其他master节点：</p>

<p>$ for NODE in k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    for FILE in admin.conf controller-manager.conf scheduler.conf; do
      scp ${K8S_DIR}/${FILE} ${NODE}:${K8S_DIR}/${FILE}
    done
  done</p>

<h2 id="kubernetes-masters">Kubernetes Masters</h2>

<p>本节将说明如何部署与设定Kubernetes Master角色中的各组件，在开始前先简单了解一下各组件功能：</p>

<ul>
<li><strong>kubelet</strong>：负责管理容器的生命周期，定期从 API Server 取得节点上的预期状态(如网络、储存等等配置)资源，并呼叫对应的容器接口(CRI、CNI 等)来达成这个状态。任何 Kubernetes 节点都会拥有该组件。</li>
<li><strong>kube-apiserver</strong>：以 REST APIs 提供 Kubernetes 资源的 CRUD，如授权、认证、访问控制与 API 注册等机制。</li>
<li><strong>kube-controller-manager</strong>：通过核心控制循环(Core Control Loop)监听 Kubernetes API 的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、Namespace Controller 等等。而这些控制器会处理着自动扩展、滚动更新等等功能。</li>
<li><strong>kube-scheduler</strong>：负责将一个(或多个)容器依据排程策略分配到对应节点上让容器引擎(如 Docker)执行。而排程受到 QoS 要求、软硬件约束、亲和性(Affinity)等等规范影响。</li>
<li><strong>Etcd</strong>：用来保存集群所有状态的 Key/Value 储存系统，所有 Kubernetes 组件会通过 API Server 来跟 Etcd 进行沟通来保存或取得资源状态。</li>
<li><strong>HAProxy</strong>：提供多个 API Server 的负载平衡(Load Balance)。</li>
<li><strong>Keepalived</strong>：建立一个虚拟 IP(VIP) 来作为 API Server 统一存取端点。</li>
</ul>

<p>而上述组件除了 kubelet 外，其他将通过 kubelet 以 <a href="https://kubernetes.io/docs/tasks/administer-cluster/static-pod/">Static Pod</a> 方式进行部署，这种方式可以减少管理 Systemd 的服务，并且能通过 kubectl 来观察启动的容器状况。</p>

<h3 id="部署与设定">部署与设定</h3>

<p>在首先k8s-m1节点展示进入k8s-manual-files目录，并依序执行下述指令来完成部署：</p>

<p>$ cd ~/k8s-manual-files</p>

<p>首先利用./hack/gen-configs.sh脚本在每台master节点产生组态文件：</p>

<p>$ export NODES=&ldquo;k8s-m1 k8s-m2 k8s-m3&rdquo;
$ ./hack/gen-configs.sh
k8s-m1 config generated&hellip;
k8s-m2 config generated&hellip;
k8s-m3 config generated&hellip;</p>

<p>后完成检查记得/etc/etcd/config.yml与/etc/haproxy/haproxy.cfg是否设定正确。</p>

<blockquote>
<p>这边主要确认文件案中的${xxx}字符串是否有被更改，并且符合环境。详细内容可以查看k8s-manual-files。</p>
</blockquote>

<p>接着利用./hack/gen-manifests.sh脚本在每台master节点产生 Static pod YAML 文件，以及其他相关配置文件(如 EncryptionConfig)：</p>

<p>$ export NODES=&ldquo;k8s-m1 k8s-m2 k8s-m3&rdquo;
$ ./hack/gen-manifests.sh
k8s-m1 manifests generated&hellip;
k8s-m2 manifests generated&hellip;
k8s-m3 manifests generated&hellip;</p>

<p>完成后记得检查/etc/kubernetes/manifests，/etc/kubernetes/encryption与/etc/kubernetes/audit目录中的文件是否的英文定正确。</p>

<blockquote>
<p>这边主要确认文件中的${xxx}字符串是否有被更改，并且符合环境需求。详细内容可以查看k8s-manual-files。</p>
</blockquote>

<p>确认上述两个产生文件步骤完成后，即可设定所有master节点的 kubelet systemd 来启动 Kubernetes 组件。首先复制下列文件到指定路径：</p>

<p>$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    ssh ${NODE} &ldquo;mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d&rdquo;
    scp master/var/lib/kubelet/config.yml ${NODE}:/var/lib/kubelet/config.yml
    scp master/systemd/kubelet.service ${NODE}:/lib/systemd/system/kubelet.service
    scp master/systemd/10-kubelet.conf ${NODE}:/etc/systemd/system/kubelet.service.d/10-kubelet.conf
  done</p>

<p>接着在k8s-m1通过SSH启动所有master节点的kubelet：</p>

<p>$ for NODE in k8s-m1 k8s-m2 k8s-m3; do
    ssh ${NODE} &ldquo;systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service&rdquo;
  done</p>

<p>完成后会需要一段时间来下载映像档与启动组件，可以利用该指令来监看：</p>

<p>$ watch netstat -ntlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      9407/kube-scheduler
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      9338/kube-controlle
tcp        0      0 127.0.0.1:38420         0.0.0.0:*               LISTEN      8676/kubelet
tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      9602/haproxy
tcp        0      0 0.0.0.0:9090            0.0.0.0:*               LISTEN      9602/haproxy
tcp6       0      0 :::10250                :::*                    LISTEN      8676/kubelet
tcp6       0      0 :::2379                 :::*                    LISTEN      9487/etcd
tcp6       0      0 :::6443                 :::*                    LISTEN      9133/kube-apiserver
tcp6       0      0 :::2380                 :::*                    LISTEN      9487/etcd
&hellip;</p>

<blockquote>
<p>若看到以上资讯表示服务正常启动，发生若问题可以用docker指令来查看。</p>
</blockquote>

<p>接下来将建立TLS Bootstrapping来让Node签证并授权注册到集群。</p>

<h3 id="建立-tls-bootstrapping">建立 TLS Bootstrapping</h3>

<p>由于本教程采用 TLS 认证来确保 Kubernetes 集群的安全性，因此每个节点的 kubelet 都需要通过 API Server 的 CA 进行身份验证后，才能与 API Server 进行沟通，而这过程过去都是采用手动方式针对每台节点(master与node)单独签署凭证，再设定给 kubelet 使用，然而这种方式是一件繁琐的事情，因为当节点扩展到一定程度时，将会非常费时，甚至延伸初管理不易问题。</p>

<p>而由于上述问题，Kubernetes 实现了 TLS Bootstrapping 来解决此问题，这种做法是先让 kubelet 以一个低权限使用者(一个能存取 CSR API 的 Token)存取 API Server，接着对 API Server 提出申请凭证签署请求，并在受理后由 API Server 动态签署 kubelet 凭证提供给对应的node节点使用。具体作法请参考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/">TLS Bootstrapping</a> 与 <a href="https://kubernetes.io/docs/admin/bootstrap-tokens/">Authenticating with Bootstrap Tokens</a>。</p>

<p>在k8s-m1建立bootstrap使用者的kubeconfig：</p>

<p>$ export TOKEN_ID=$(openssl rand 3 -hex)
$ export TOKEN_SECRET=$(openssl rand 8 -hex)
$ export BOOTSTRAP_TOKEN=${TOKEN_ID}.${TOKEN_SECRET}
$ export KUBE_APISERVER=&ldquo;<a href="https://172.22.132.9:6443&quot;">https://172.22.132.9:6443&quot;</a></p>

<p>$ kubectl config set-cluster kubernetes
    &ndash;certificate-authority=/etc/kubernetes/pki/ca.pem
    &ndash;embed-certs=true
    &ndash;server=${KUBE_APISERVER}
    &ndash;kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</p>

<p>$ kubectl config set-credentials tls-bootstrap-token-user
    &ndash;token=${BOOTSTRAP_TOKEN}
    &ndash;kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</p>

<p>$ kubectl config set-context tls-bootstrap-token-user@kubernetes
    &ndash;cluster=kubernetes
    &ndash;user=tls-bootstrap-token-user
    &ndash;kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</p>

<p>$ kubectl config use-context tls-bootstrap-token-user@kubernetes
    &ndash;kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf</p>

<blockquote>
<p>KUBE_APISERVER这边设定为 VIP 地址。若想要用手动签署凭证来进行授权的话，可以参考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/">Certificate</a>。</p>
</blockquote>

<p>接着在k8s-m1建立TLS Bootstrap Secret来提供自动签证使用：</p>

<p>$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-${TOKEN_ID}
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  token-id: &ldquo;${TOKEN_ID}&rdquo;
  token-secret: &ldquo;${TOKEN_SECRET}&rdquo;
  usage-bootstrap-authentication: &ldquo;true&rdquo;
  usage-bootstrap-signing: &ldquo;true&rdquo;
  auth-extra-groups: system:bootstrappers:default-node-token
EOF</p>

<p>secret &ldquo;bootstrap-token-65a3a9&rdquo; created</p>

<p>然后建立TLS Bootstrap Autoapprove RBAC来提供自动受理CSR：</p>

<p>$ kubectl apply -f master/resources/kubelet-bootstrap-rbac.yml
clusterrolebinding.rbac.authorization.k8s.io/kubelet-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-bootstrap created
clusterrolebinding.rbac.authorization.k8s.io/node-autoapprove-certificate-rotation created</p>

<h3 id="验证master节点">验证Master节点</h3>

<p>完成后，在任意一台master节点复制Admin kubeconfig文件，并通过简单指令验证：</p>

<p>$ cp /etc/kubernetes/admin.conf ~/.kube/config
$ kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {&ldquo;health&rdquo;:&ldquo;true&rdquo;}
etcd-1               Healthy   {&ldquo;health&rdquo;:&ldquo;true&rdquo;}
etcd-2               Healthy   {&ldquo;health&rdquo;:&ldquo;true&rdquo;}</p>

<p>$ kubectl -n kube-system get po
NAME                             READY     STATUS    RESTARTS   AGE
etcd-k8s-m1                      <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1h
etcd-k8s-m2                      <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1h
etcd-k8s-m3                      <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1h
kube-apiserver-k8s-m1            <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1h
kube-apiserver-k8s-m2            <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1h
kube-apiserver-k8s-m3            <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1h
&hellip;</p>

<p>$ kubectl get node
NAME      STATUS     ROLES     AGE       VERSION
k8s-m1    NotReady   master    38s       v1.11.0
k8s-m2    NotReady   master    37s       v1.11.0
k8s-m3    NotReady   master    36s       v1.11.0</p>

<blockquote>
<p>这在阶段状态处于NotReady的英文正常，往下进行就会了解为何。</p>
</blockquote>

<p>通过kubectl logs来查看容器的日志：</p>

<p>$ kubectl -n kube-system logs -f kube-apiserver-k8s-m1
Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-k8s-m1)</p>

<blockquote>
<p>这边会发现出现 403 Forbidden 问题，这是因为 kube-apiserver user 并没有 nodes 的资源访问权限，属于正常。</p>
</blockquote>

<p>为了方便管理集群，因此需要通过 kubectl logs 来查看，但由于 API 权限问题，故需要建立一个 RBAC Role 来获取访问权限，这边在k8s-m1节点执行以下指令建立：</p>

<p>$ kubectl apply -f master/resources/apiserver-to-kubelet-rbac.yml
clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created
clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created</p>

<p>完成后，再次通过kubectl logs查看Pod：</p>

<p>$ kubectl -n kube-system logs -f kube-apiserver-k8s-m1
I0708 15:22:33.906269       1 get.go:245] Starting watch for /api/v1/services, rv=2494 labels= fields= timeout=8m29s
I0708 15:22:40.919638       1 get.go:245] Starting watch for /apis/certificates.k8s.io/v1beta1/certificatesigningrequests, rv=11084 labels= fields= timeout=7m29s
&hellip;</p>

<p>接着设定 <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations</a> 来让一些特定 Pod 能够排程到所有master节点上：</p>

<p>$ kubectl taint nodes node-role.kubernetes.io/master=&ldquo;&rdquo;:NoSchedule &ndash;all
node &ldquo;k8s-m1&rdquo; tainted
node &ldquo;k8s-m2&rdquo; tainted
node &ldquo;k8s-m3&rdquo; tainted</p>

<p>这边截至已完成master节点部署，将接下来针对node的部署进行说明。</p>

<h2 id="kubernetes-nodes">Kubernetes Nodes</h2>

<p>本节将说明如何建立与设定 Kubernetes Node 节点，Node 是主要执行容器实例(Pod)的工作节点。这过程只需要将 PKI、Bootstrap conf 等文件复制到机器上，再用 kubelet 启动即可。</p>

<p>在开始部署前，在k8-m1将需要用到的文件复制到所有node节点上：</p>

<p>$ for NODE in k8s-g1 k8s-g2; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    ssh ${NODE} &ldquo;mkdir -p /etc/kubernetes/pki/&rdquo;
    for FILE in pki/ca.pem pki/ca-key.pem bootstrap-kubelet.conf; do
      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}
    done
  done</p>

<h3 id="部署与设定-1">部署与设定</h3>

<p>确认文件都复制后，即可设定所有node节点的 kubelet systemd 来启动 Kubernetes 组件。首先在k8s-m1复制下列文件到指定路径：</p>

<p>$ cd ~/k8s-manual-files
$ for NODE in k8s-g1 k8s-g2; do
    echo &ldquo;&mdash; $NODE &mdash;&rdquo;
    ssh ${NODE} &ldquo;mkdir -p /var/lib/kubelet /var/log/kubernetes /var/lib/etcd /etc/systemd/system/kubelet.service.d /etc/kubernetes/manifests&rdquo;
    scp node/var/lib/kubelet/config.yml ${NODE}:/var/lib/kubelet/config.yml
    scp node/systemd/kubelet.service ${NODE}:/lib/systemd/system/kubelet.service
    scp node/systemd/10-kubelet.conf ${NODE}:/etc/systemd/system/kubelet.service.d/10-kubelet.conf
  done</p>

<p>接着在k8s-m1通过SSH启动所有node节点的kubelet：</p>

<p>$ for NODE in k8s-g1 k8s-g2; do
    ssh ${NODE} &ldquo;systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service&rdquo;
  done</p>

<h3 id="验证node节点">验证Node节点</h3>

<p>完成后，在任意一台master节点复制Admin kubeconfig文件，并通过简单指令验证：</p>

<p>$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-99n76                                              1h        system:node:k8s-m2        Approved,Issued
csr-9n88h                                              1h        system:node:k8s-m1        Approved,Issued
csr-vdtqr                                              1h        system:node:k8s-m3        Approved,Issued
node-csr-5VkCjWvb8tGVtO-d2gXiQrnst-G1xe_iA0AtQuYNEMI   2m        system:bootstrap:872255   Approved,Issued
node-csr-Uwpss9OhJrAgOB18P4OIEH02VHJwpFrSoMOWkkrK-lo   2m        system:bootstrap:872255   Approved,Issued</p>

<p>$ kubectl get nodes
NAME      STATUS     ROLES     AGE       VERSION
k8s-g1    NotReady   <none>    8m        v1.11.0
k8s-g2    NotReady   <none>    8m        v1.11.0
k8s-m1    NotReady   master    20m       v1.11.0
k8s-m2    NotReady   master    20m       v1.11.0
k8s-m3    NotReady   master    20m       v1.11.0</p>

<blockquote>
<p>这在阶段状态处于NotReady的英文正常，往下进行就会了解为何。</p>
</blockquote>

<p>到这边就表示node节点部署已完成了，接下来章节将针对Kubernetes Addons安装进行说明。</p>

<h2 id="kubernetes-core-addons部署">Kubernetes Core Addons部署</h2>

<p>当完成master与node节点的部署，并组合成一个可运作集群后，就可以开始通过 kubectl 部署 Addons，Kubernetes 官方提供了多种 Addons 来加强 Kubernetes 的各种功能，如集群 DNS 解析的kube-dns(or CoreDNS)、外部存取服务的kube-proxy与 Web-based 管理接口的dashboard等等。而其中有些 Addons 是被 Kubernetes 认定为必要的，因此本节将说明如何部署这些 Addons。</p>

<p>在首先k8s-m1节点展示进入k8s-manual-files目录，并依序执行下述指令来完成部署：</p>

<p>$ cd ~/k8s-manual-files</p>

<h3 id="kubernetes-proxy">Kubernetes Proxy</h3>

<p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy">kube-proxy</a> 是实现 Kubernetes Service 资源功能的关键组件，这个组件会通过 DaemonSet 在每台节点上执行，然后监听 API Server 的 Service 与 Endpoint 资源对象的事件，并依据资源预期状态通过 iptables 或 ipvs 来实现网络转发，而本次安装采用 ipvs。</p>

<p>在k8s-m1通过kubeclt执行下面指令来建立，并检查是否部署成功：</p>

<p>$ export KUBE_APISERVER=<a href="https://172.22.132.9:6443">https://172.22.132.9:6443</a>
$ sed -i &ldquo;s/${KUBE_APISERVER}/${KUBE_APISERVER}/g&rdquo;
$ kubectl -f addons/kube-proxy/</p>

<p>$ kubectl -n kube-system get po -l k8s-app=kube-proxy
NAME               READY     STATUS    RESTARTS   AGE
kube-proxy-dd2m7   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          8m
kube-proxy-fwgx8   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          8m
kube-proxy-kjn57   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          8m
kube-proxy-vp47w   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          8m
kube-proxy-xsncw   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          8m</p>

<h1 id="检查-log-是否使用-ipvs">检查 log 是否使用 ipvs</h1>

<p>$ kubectl -n kube-system logs -f kube-proxy-fwgx8
I0709 08:41:48.220815       1 feature_gate.go:230] feature gates: &amp;{map[SupportIPVSProxyMode:true]}
I0709 08:41:48.231009       1 server_others.go:183] Using ipvs Proxier.
&hellip;</p>

<p>若有安装 ipvsadm 的话，可以通过以下指令查看 proxy 规则：</p>

<p>$ ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -&gt; 172.22.132.9:5443            Masq    1      0          0</p>

<h3 id="coredns">CoreDNS</h3>

<p>本节将通过 CoreDNS 取代 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns">Kube DNS</a> 作为集群服务发现组件，由于 Kubernetes 需要让 Pod 与 Pod 之间能够互相沟通，然而要能够沟通需要知道彼此的 IP 才行，而这种做法通常是通过 Kubernetes API 来取得达到，但是 Pod IP 会因为生命周期变化而改变，因此这种做法无法弹性使用，且还会增加 API Server 负担，基于此问题 Kubernetes 提供了 DNS 服务来作为查询，让 Pod 能够以 Service 名称作为域名来查询 IP 地址，因此用户就再不需要关切实际 Pod IP，而 DNS 也会根据 Pod 变化更新资源纪录(Record resources)。</p>

<p><a href="https://github.com/coredns/coredns">CoreDNS</a> 是由 CNCF 维护的开源 DNS 项目，该项目前身是 SkyDNS，其采用了 Caddy 的一部分来开发服务器框架，使其能够建构一套快速灵活的 DNS，而 CoreDNS 每个功能都可以被实作成一个插件的中间件，如 Log、Cache、Kubernetes 等功能，甚至能够将源纪录储存至 Redis、Etcd 中。</p>

<p>在k8s-m1通过kubeclt执行下面指令来建立，并检查是否部署成功：</p>

<p>$ kubectl create -f coredns/</p>

<p>$ kubectl -n kube-system get po -l k8s-app=kube-dns
NAME                       READY     STATUS    RESTARTS   AGE
coredns-589dd74cb6-5mv5c   0/1       Pending   0          3m
coredns-589dd74cb6-d42ft   0/1       Pending   0          3m</p>

<p>这边会发现 Pod 处于Pending状态，这是由于 Kubernetes 的集群网络没有建立，因此所有节点会处于NotReady状态，而这也导致 Kubernetes Scheduler 无法替 Pod 找到适合节点而处于Pending，为了解决这个问题，下节将说明与建立 Kubernetes 集群网络。</p>

<blockquote>
<p>若 Pod 是被 DaemonSet 管理的话，则不会 Pending，不过若没有设定hostNetwork则会出问题。</p>
</blockquote>

<h2 id="kubernetes集群网路">Kubernetes集群网路</h2>

<p>Kubernetes 在默认情况下与 Docker 的网络有所不同。在 Kubernetes 中有四个问题是需要被解决的，分别为：</p>

<ul>
<li><strong>高耦合的容器到容器沟通</strong>：通过 Pods 与 Localhost 的沟通来解决。</li>
<li><strong>Pod</strong> <strong>到</strong> <strong>Pod</strong> <strong>的沟通</strong>：通过实现网络模型来解决。</li>
<li><strong>Pod</strong> <strong>到</strong> <strong>Service</strong> <strong>沟通</strong>：由 Services object 结合 kube-proxy 解决。</li>
<li><strong>外部到</strong> <strong>Service</strong> <strong>沟通</strong>：一样由 Services object 结合 kube-proxy 解决。</li>
</ul>

<p>而 Kubernetes 对于任何网络的实现都需要满足以下基本要求(除非是有意调整的网络分段策略)：</p>

<ul>
<li>所有容器能够在没有 NAT 的情况下与其他容器沟通。</li>
<li>所有节点能够在没有 NAT 情况下与所有容器沟通(反之亦然)。</li>
<li>容器看到的 IP 与其他人看到的 IP 是一样的。</li>
</ul>

<p>庆幸的是 Kubernetes 已经有非常多种的<a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">网络模型</a>以<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件(Network Plugins)</a>方式被实现，因此可以选用满足自己需求的网络功能来使用。另外 Kubernetes 中的网络插件有以下两种形式：</p>

<ul>
<li><strong>CNI plugins</strong>：以 appc/CNI 标准规范所实现的网络，详细可以阅读 <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI Specification</a>。</li>
<li><strong>Kubenet plugin</strong>：使用 CNI plugins 的 bridge 与 host-local 来实现基本的 cbr0。这通常被用在公有云服务上的 Kubernetes 集群网络。</li>
</ul>

<blockquote>
<p>如果了解如何选择可以阅读 Chris Love 的 <a href="https://chrislovecnm.com/kubernetes/cni/choosing-a-cni-provider/">Choosing a CNI Network Provider for Kubernetes</a> 文章。</p>
</blockquote>

<h3 id="网路部署与设定">网路部署与设定</h3>

<p>从上述了解 Kubernetes 有多种网络能够选择，而本教学选择了 <a href="https://www.projectcalico.org/">Calico</a> 作为集群网络的使用。Calico 是一款纯 Layer 3 的网络，其好处是它整合了各种云原生平台(Docker、Mesos 与 OpenStack 等)，且 Calico 不采用 vSwitch，而是在每个 Kubernetes 节点使用 vRouter 功能，并通过 Linux Kernel 既有的 L3 forwarding 功能，而当数据中心复杂度增加时，Calico 也可以利用 BGP route reflector 来达成。</p>

<blockquote>
<p>想了解 Calico 与传统 overlay networks 的差异，可以阅读 <a href="https://www.projectcalico.org/learn/">Difficulties with traditional overlay networks</a> 文章。</p>
</blockquote>

<p>由于 Calico 提供了 Kubernetes resources YAML 文件来快速以容器方式部署网络插件至所有节点上，因此只需要在k8s-m1通过 kubeclt 执行下面指令来建立：</p>

<p>$ cd ~/k8s-manual-files
$ sed -i &rsquo;s/192.168.0.0/16/10.244.0.0/16/g&rsquo; cni/calico/v3.1/calico.yaml
$ kubectl -f cni/calico/v3.1/</p>

<blockquote>
<ul>
<li>这边要记得将CALICO_IPV4POOL_CIDR的网络修改 Cluster IP CIDR。</li>
<li>另外当节点超过 50 台，可以使用 Calico 的 <a href="https://github.com/projectcalico/typha">Typha</a> 模式来减少通过 Kubernetes datastore 造成 API Server 的负担。</li>
</ul>
</blockquote>

<p>部署后通过kubectl检查是否有启动：</p>

<p>$ kubectl -n kube-system get po -l k8s-app=calico-node
NAME                READY     STATUS    RESTARTS   AGE
calico-node-27jwl   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
calico-node-4fgv6   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
calico-node-mvrt7   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
calico-node-p2q9g   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
calico-node-zchsz   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s</p>

<p>确认 calico-node 都正常运作后，通过 kubectl exec 进入 calicoctl pod 来检查功能是否正常：</p>

<p>$ kubectl exec -ti -n kube-system calicoctl &ndash; calicoctl get profiles -o wide
NAME              LABELS
kns.default       map[]
kns.kube-public   map[]
kns.kube-system   map[]</p>

<p>$ kubectl exec -ti -n kube-system calicoctl &ndash; calicoctl get node -o wide
NAME     ASN         IPV4               IPV6
k8s-g1   (unknown)   172.22.132.<sup>13</sup>&frasl;<sub>24</sub>
k8s-g2   (unknown)   172.22.132.<sup>14</sup>&frasl;<sub>24</sub>
k8s-m1   (unknown)   172.22.132.<sup>10</sup>&frasl;<sub>24</sub>
k8s-m2   (unknown)   172.22.132.<sup>11</sup>&frasl;<sub>24</sub>
k8s-m3   (unknown)   172.22.132.<sup>12</sup>&frasl;<sub>24</sub></p>

<blockquote>
<p>若没问题，就可以将kube-system下的calicoctl pod删除。</p>
</blockquote>

<p>完成后，通过检查节点是否不再是NotReady，以及Pod是否不再处于Pending：</p>

<p>$ kubectl get no
NAME      STATUS    ROLES     AGE       VERSION
k8s-g1    Ready     <none>    35m       v1.11.0
k8s-g2    Ready     <none>    35m       v1.11.0
k8s-m1    Ready     master    35m       v1.11.0
k8s-m2    Ready     master    35m       v1.11.0
k8s-m3    Ready     master    35m       v1.11.0</p>

<p>$ kubectl -n kube-system get po -l k8s-app=kube-dns -o wide
NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
coredns-589dd74cb6-5mv5c   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          10m       10.244.4.2   k8s-g2
coredns-589dd74cb6-d42ft   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          10m       10.244.3.2   k8s-g1</p>

<p>当成功到这边时，一个能运作的 Kubernetes 集群基本上就完成了，接下来将介绍一些好用的 Addons 来帮助使用与管理 Kubernetes。</p>

<h2 id="kubernetes-extra-addons部署">Kubernetes Extra Addons部署</h2>

<p>本节说明如何部署一些官方常用的额外 Addons，如 Dashboard、Metrics Server 与 Ingress Controller 等等。</p>

<p>所有 Addons 部署文件均存已放至k8s-manual-files中，因此在k8s-m1进入该目录，并依序下小节建立：</p>

<p>$ cd ~/k8s-manual-files</p>

<h3 id="ingress-controller">Ingress Controller</h3>

<p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> 是 Kubernetes 中的一个抽象资源，其功能是通过 Web Server 的 Virtual Host 概念以域名(Domain Name)方式转发到内部 Service，这避免了使用 Service 中的 NodePort 与 LoadBalancer 类型所带来的限制(如 Port 数量上限)，而实现 Ingress 功能则是通过 <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers">Ingress Controller</a> 来达成，它会负责监听 Kubernetes API 中的 Ingress 与 Service 资源对象，并在发生资源变化时，依据资源预期的结果来设定 Web Server。另外 Ingress Controller 有许多实现可以选择：</p>

<ul>
<li><a href="https://github.com/kubernetes/ingress-nginx">Ingress NGINX</a>: Kubernetes 官方维护的项目，也是本次安装使用的 Controller。</li>
<li><a href="https://clouddocs.f5.com/products/connectors/k8s-bigip-ctlr/v1.5/">F5 BIG-IP Controller</a>: F5 所开发的 Controller，它能够让管理员通过 CLI 或 API 从 Kubernetes 与 OpenShift 管理 F5 BIG-IP 设备。</li>
<li><a href="https://konghq.com/blog/kubernetes-ingress-controller-for-kong/">Ingress Kong</a>: 著名的开源 API Gateway 项目所维护的 Kubernetes Ingress Controller。</li>
<li><a href="https://github.com/containous/traefik">Træfik</a>: 是一套开源的 HTTP 反向代理与负载平衡器，而它也支持了 Ingress。</li>
<li><a href="https://github.com/appscode/voyager">Voyager</a>: 一套以 HAProxy 为底的 Ingress Controller。</li>
</ul>

<blockquote>
<p>而 Ingress Controller 的实现不只这些项目，还有很多可以在网络上找到，未来自己也会写一篇 Ingress Controller 的实作方式文章。</p>
</blockquote>

<p>首先在k8s-m1执行下述指令来建立 Ingress Controller，并检查是否部署正常：</p>

<p>$ export INGRESS_VIP=172.22.132.8
$ sed -i &ldquo;s/${INGRESS_VIP}/${INGRESS_VIP}/g&rdquo; addons/ingress-controller/ingress-controller-svc.yml
$ kubectl create ns ingress-nginx
$ kubectl apply -f addons/ingress-controller
$ kubectl -n ingress-nginx get po,svc
NAME                                           READY     STATUS    RESTARTS   AGE
pod/default-http-backend-846b65fb5f-l5hrc      <sup>1</sup>&frasl;<sub>1</sub>       Running   0          2m
pod/nginx-ingress-controller-5db8d65fb-z2lf9   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          2m</p>

<p>NAME                           TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
service/default-http-backend   ClusterIP      10.99.105.112   <none>         80/TCP         2m
service/ingress-nginx          LoadBalancer   10.106.18.106   172.22.132.8   80:31197/TCP   2m</p>

<p>后完成浏览通过器存取<a href="http://172.22.132.8/">http://172.22.132.8:80</a>来查看是否能连线，若可以会如下图结果。</p>

<p><img src="https://i.imgur.com/CfbLwOP.png" alt="" /></p>

<p>当确认上面步骤都没问题后，就可以通过kubeclt建立简单NGINX来测试功能：</p>

<p>$ kubectl apply -f apps/nginx/
deployment.extensions/nginx created
ingress.extensions/nginx-ingress created
service/nginx created</p>

<p>$ kubectl get po,svc,ing
NAME                        READY     STATUS    RESTARTS   AGE
pod/nginx-966857787-78kth   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          32s</p>

<p>NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   2d
service/nginx        ClusterIP   10.104.180.119   <none>        80/TCP    32s</p>

<p>NAME                               HOSTS             ADDRESS        PORTS     AGE
ingress.extensions/nginx-ingress   nginx.k8s.local   172.22.132.8   80        33s</p>

<blockquote>
<p>P.S. Ingress 规则也支持不同 Path 的服务转发，可以参考上面提供的官方文件来设定。</p>
</blockquote>

<p>完成后通过cURL工具来测试功能是否正常：</p>

<p>$ curl 172.22.132.8 -H &lsquo;Host: nginx.k8s.local&rsquo;
&lt;!DOCTYPE html&gt;
<html>
<head>
<title>Welcome to nginx!</title>
&hellip;</p>

<h1 id="测试其他-domain-name-是是否回应-404">测试其他 domain name 是是否回应 404</h1>

<p>$ curl 172.22.132.8 -H &lsquo;Host: nginx1.k8s.local&rsquo;
default backend - 404</p>

<p>虽然 Ingress 能够让我们通过域名方式存取 Kubernetes 内部服务，但是若域名于法被测试机器解析的话，将会显示default backend – 404结果，而这经常发生在内部自建环境上，虽然可以通过修改主机/etc/hosts来描述，但并不弹性，因此下节将说明如何建立一个 External DNS 与 DNS 服务器来提供自动解析 Ingress 域名。</p>

<h3 id="external-dns">External DNS</h3>

<p><a href="https://github.com/kubernetes-incubator/external-dns">External DNS</a> 是 Kubernetes 小区的孵化项目，被用于定期同步 Kubernetes Service 与 Ingress 资源，并依据资源内容来自动设定公有云 DNS 服务的资源纪录(Record resources)。而由于部署不是公有云环境，因此需要通过 CoreDNS 提供一个内部 DNS 服务器，再由 ExternalDNS 与这个 CoreDNS 做串接。</p>

<p>首先在k8s-m1执行下述指令来建立CoreDNS Server，并检查是否部署正常：</p>

<p>$ export DNS_VIP=172.22.132.8
$ sed -i &ldquo;s/${DNS_VIP}/${DNS_VIP}/g&rdquo; addons/external-dns/coredns/coredns-svc-tcp.yml
$ sed -i &ldquo;s/${DNS_VIP}/${DNS_VIP}/g&rdquo; addons/external-dns/coredns/coredns-svc-udp.yml
$ kubectl create -f addons/external-dns/coredns/
$ kubectl -n external-dns get po,svc
NAME                                READY     STATUS    RESTARTS   AGE
pod/coredns-54bcfcbd5b-5grb5        <sup>1</sup>&frasl;<sub>1</sub>       Running   0          2m
pod/coredns-etcd-6c9c68fd76-n8rhj   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          2m</p>

<p>NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)                       AGE
service/coredns-etcd   ClusterIP      10.110.186.83    <none>         2379/TCP,2380/TCP             2m
service/coredns-tcp    LoadBalancer   10.109.105.166   172.22.132.8   53:32169/TCP,9153:32150/TCP   2m
service/coredns-udp    LoadBalancer   10.110.242.185   172.22.132.8   53:31210/UDP</p>

<blockquote>
<p>这边域名为k8s.local，修改可以文件中的coredns-cm.yml来改变。</p>
</blockquote>

<p>完成后，通过dig 工具来检查是否DNS是否正常：</p>

<p>$ dig @172.22.132.8 SOA nginx.k8s.local +noall +answer +time=2 +tries=1
&hellip;
; (1 server found)
;; global options: +cmd
k8s.local.        300    IN    SOA    ns.dns.k8s.local. hostmaster.k8s.local. 1531299150 7200 1800 86400 30</p>

<p>接着部署ExternalDNS来与CoreDNS同步资源纪录：</p>

<p>$ kubectl apply -f addons/external-dns/external-dns/
$ kubectl -n external-dns get po -l k8s-app=external-dns
NAME                            READY     STATUS    RESTARTS   AGE
external-dns-86f67f6df8-ljnhj   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1m</p>

<p>完成后，通过dig 与nslookup工具检查上节测试Ingress的NGINX服务：</p>

<p>$ dig @172.22.132.8 A nginx.k8s.local +noall +answer +time=2 +tries=1
&hellip;
; (1 server found)
;; global options: +cmd
nginx.k8s.local.    300    IN    A    172.22.132.8</p>

<p>$ nslookup nginx.k8s.local
Server:        172.22.132.8
Address:    172.22.132.8#53</p>

<p>** server can&rsquo;t find nginx.k8s.local: NXDOMAIN</p>

<p>这时会无法通过 nslookup 解析域名，这是因为测试机器并没有使用这个 DNS 服务器，可以通过修改/etc/resolv.conf来加入，或者类似下图方式(不同 OS 有差异，不过都在网络设定中改)。</p>

<p><img src="https://i.imgur.com/MVDhXKi.png" alt="" /></p>

<p>再次通过nslookup检查，会发现可以解析了，这时也就能通过cURL来测试结果：</p>

<p>$ nslookup nginx.k8s.local
Server:        172.22.132.8
Address:    172.22.132.8#53</p>

<p>Name:    nginx.k8s.local
Address: 172.22.132.8</p>

<p>$ curl nginx.k8s.local
&lt;!DOCTYPE html&gt;
<html>
<head>
<title>Welcome to nginx!</title>
&hellip;</p>

<h3 id="dashboard">Dashboard</h3>

<p><a href="https://github.com/kubernetes/dashboard">Dashboard</a> 是 Kubernetes 官方开发的 Web-based 仪表板，目的是提升管理 Kubernetes 集群资源便利性，并以资源可视化方式，来让人更直觉的看到整个集群资源状态，</p>

<p>在k8s-m1通过 kubeclt 执行下面指令来建立 Dashboard 至 Kubernetes，并检查是否正确部署：</p>

<p>$ cd ~/k8s-manual-files
$ kubectl apply -f addons/dashboard/
$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboard
NAME                                       READY     STATUS    RESTARTS   AGE
pod/kubernetes-dashboard-6948bdb78-w26qc   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          2m</p>

<p>NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/kubernetes-dashboard   ClusterIP   10.109.31.80   <none>        443/TCP   2m</p>

<p>在这边会额外建立名称为anonymous-dashboard-proxy的 Cluster Role(Binding) 来让system:anonymous这个匿名用户能够通过 API Server 来 proxy 到 Kubernetes Dashboard，而这个 RBAC 规则仅能够存取services/proxy资源，以及https:kubernetes-dashboard:资源名称。</p>

<p>因此我们能够在完成后，通过以下连结来进入 Kubernetes Dashboard：</p>

<ul>
<li>https://{YOUR_VIP}:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</li>
</ul>

<p>由于 Kubernetes Dashboard v1.7 版本以后不再提供 Admin 权限，因此需要通过 kubeconfig 或者 Service Account 来进行登入才能取得资源来呈现，这边建立一个 Service Account 来绑定cluster-admin 以测试功能：</p>

<p>$ kubectl -n kube-system create sa dashboard
$ kubectl create clusterrolebinding dashboard &ndash;clusterrole cluster-admin &ndash;serviceaccount=kube-system:dashboard
$ SECRET=$(kubectl -n kube-system get sa dashboard -o yaml | awk &lsquo;/dashboard-token/ {print $3}&rsquo;)
$ kubectl -n kube-system describe secrets ${SECRET} | awk &lsquo;/token:/{print $2}&rsquo;
eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtdG9rZW4tdzVocmgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWJmMTFjYzMtZjRlYi0xMWU3LTgzYWUtMDgwMDI3NjdkOWI5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZCJ9.Xuyq34ci7Mk8bI97o4IldDyKySOOqRXRsxVWIJkPNiVUxKT4wpQZtikNJe2mfUBBD-JvoXTzwqyeSSTsAy2CiKQhekW8QgPLYelkBPBibySjBhJpiCD38J1u7yru4P0Pww2ZQJDjIxY4vqT46ywBklReGVqY3ogtUQg-eXueBmz-o7lJYMjw8L14692OJuhBjzTRSaKW8U2MPluBVnD7M2SOekDff7KpSxgOwXHsLVQoMrVNbspUCvtIiEI1EiXkyCNRGwfnd2my3uzUABIHFhm0_RZSmGwExPbxflr8Fc6bxmuz-_jSdOtUidYkFIzvEWw2vRovPgs3MXTv59RwUw</p>

<blockquote>
<p>复制token然后贴到 Kubernetes dashboard。注意这边一般来说要针对不同 User 开启特定访问权限。</p>
</blockquote>

<p><img src="https://kairen.github.io/images/kube/kubernetes-dashboard.png" alt="" /></p>

<h3 id="prometheus">Prometheus</h3>

<p>由于 <a href="https://github.com/kubernetes/heapster/blob/master/docs/deprecation.md">Heapster</a> 将要被移弃，因此这边选用 <a href="https://prometheus.io/">Prometheus</a> 作为第三方的集群监控方案。而本次安装采用 CoreOS 开发的 <a href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a> 用于管理在 Kubernetes 上的 Prometheus 集群与资源，更多关于 Prometheus Operator 的信息可以参考小弟的 <a href="https://kairen.github.io/2018/06/23/devops/prometheus-operator/">Prometheus Operator 介绍与安装</a> 文章。</p>

<p>首先在k8s-m1执行下述指令来部署所有Prometheus需要的组件：</p>

<p>$ kubectl apply -f addons/prometheus/
$ kubectl apply -f addons/prometheus/operator/
$ kubectl apply -f addons/prometheus/alertmanater/
$ kubectl apply -f addons/prometheus/node-exporter/
$ kubectl apply -f addons/prometheus/kube-state-metrics/
$ kubectl apply -f addons/prometheus/grafana/
$ kubectl apply -f addons/prometheus/kube-service-discovery/
$ kubectl apply -f addons/prometheus/prometheus/
$ kubectl apply -f addons/prometheus/servicemonitor/</p>

<p>完成后，通过kubectl检查服务是否正常运行：</p>

<p>$ kubectl -n monitoring get po,svc,ing
NAME                                      READY     STATUS    RESTARTS   AGE
pod/alertmanager-main-0                   <sup>1</sup>&frasl;<sub>2</sub>       Running   0          1m
pod/grafana-6d495c46d5-jpf6r              <sup>1</sup>&frasl;<sub>1</sub>       Running   0          43s
pod/kube-state-metrics-b84cfb86-4b8qg     <sup>4</sup>&frasl;<sub>4</sub>       Running   0          37s
pod/node-exporter-2f4lh                   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
pod/node-exporter-7cz5s                   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
pod/node-exporter-djdtk                   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
pod/node-exporter-kfpzt                   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
pod/node-exporter-qp2jf                   <sup>2</sup>&frasl;<sub>2</sub>       Running   0          59s
pod/prometheus-k8s-0                      <sup>3</sup>&frasl;<sub>3</sub>       Running   0          28s
pod/prometheus-k8s-1                      <sup>3</sup>&frasl;<sub>3</sub>       Running   0          15s
pod/prometheus-operator-9ffd6bdd9-rvqsz   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1m</p>

<p>NAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/alertmanager-main       ClusterIP   10.110.188.2     <none>        9093/TCP            1m
service/alertmanager-operated   ClusterIP   None             <none>        9093/TCP,6783/TCP   1m
service/grafana                 ClusterIP   10.104.147.154   <none>        3000/TCP            43s
service/kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP   51s
service/node-exporter           ClusterIP   None             <none>        9100/TCP            1m
service/prometheus-k8s          ClusterIP   10.96.78.58      <none>        9090/TCP            28s
service/prometheus-operated     ClusterIP   None             <none>        9090/TCP            33s
service/prometheus-operator     ClusterIP   10.99.251.16     <none>        8080/TCP            1m</p>

<p>NAME                                HOSTS                             ADDRESS        PORTS     AGE
ingress.extensions/grafana-ing      grafana.monitoring.k8s.local      172.22.132.8   80        45s
ingress.extensions/prometheus-ing   prometheus.monitoring.k8s.local   172.22.132.8   80        34s</p>

<p>确认没问题后，通过浏览器查看 <a href="http://prometheus.monitoring.k8s.local/">prometheus.monitoring.k8s.local</a> 与 <a href="http://grafana.monitoring.k8s.local/">grafana.monitoring.k8s.local</a> 是否正常，若没问题就可以看到如下图所示结果。</p>

<p><img src="https://i.imgur.com/XFTZ4eF.png" alt="" /></p>

<p><img src="https://i.imgur.com/YB5KAPe.png" alt="" /></p>

<blockquote>
<p>另外这边也推荐用<a href="https://github.com/weaveworks/scope">Weave Scope</a>来监控容器的网路Flow拓朴图。</p>
</blockquote>

<h3 id="metrics-server">Metrics Server</h3>

<p><a href="https://github.com/kubernetes-incubator/metrics-server">Metrics Server</a> 是实现了资源 Metrics API 的组件，其目标是取代 Heapster 作为 Pod 与 Node 提供资源的 Usage metrics，该组件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 Metrics。</p>

<p>首先在k8s-m1测试一下 kubectl top 指令：</p>

<p>$ kubectl top node
error: metrics not available yet</p>

<p>发现 top 指令无法取得 Metrics，这表示 Kubernetes 集群没有安装 Heapster 或是 Metrics Server 来提供 Metrics API 给 top 指令取得资源使用量。</p>

<p>由于上述问题，我们要在k8s-m1节点通过 kubectl 部署 Metrics Server 组件来解决：</p>

<p>$ kubectl create -f addons/metric-server/
$ kubectl -n kube-system get po -l k8s-app=metrics-server
NAME                                  READY     STATUS    RESTARTS   AGE
pod/metrics-server-86bd9d7667-5hbn6   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          1m</p>

<p>完成后，等待一点时间（约30s – 1m）收集指标，再次执行kubectl top指令查看：</p>

<p>$ kubectl top node
NAME      CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%
k8s-g1    106m         2%        1037Mi          6%
k8s-g2    212m         5%        1043Mi          8%
k8s-m1    386m         9%        2125Mi          13%
k8s-m2    320m         8%        1834Mi          11%
k8s-m3    457m         11%       1818Mi          11%</p>

<p>而这时若有使用<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a>的话，就能够正确抓到Pod的CPU与Memory使用量了。</p>

<blockquote>
<p>若想让HPA使用Prometheus的Metrics的话，可以阅读<a href="https://github.com/stefanprodan/k8s-prom-hpa#setting-up-a-custom-metrics-server">Custom Metrics Server</a>来了解。</p>
</blockquote>

<h3 id="helm-tiller-server">Helm Tiller Server</h3>

<p><a href="https://github.com/kubernetes/helm">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套预先组态的 Kubernetes 资源。其中Tiller Server主要负责接收来至 Client 的指令，并通过 kube-apiserver 与 Kubernetes 集群做沟通，根据 Chart 定义的内容，来产生与管理各种对应 API 对象的 Kubernetes 部署文件(又称为 Release)。</p>

<p>首先在k8s-m1安装Helm工具：</p>

<p>$ wget -qO- <a href="https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz">https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz</a> | tar -zx
$ sudo mv linux-amd64/helm /usr/local/bin/</p>

<p>另外在所有node节点安装socat：</p>

<p>$ sudo apt-get install -y socat</p>

<p>接着初始化Helm（这边会安装Tiller Server）：</p>

<p>$ kubectl -n kube-system create sa tiller
$ kubectl create clusterrolebinding tiller &ndash;clusterrole cluster-admin &ndash;serviceaccount=kube-system:tiller
$ helm init &ndash;service-account tiller
&hellip;
Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.
Happy Helming!</p>

<p>$ kubectl -n kube-system get po -l app=helm
NAME                            READY     STATUS    RESTARTS   AGE
tiller-deploy-759cb9df9-rfhqw   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          19s</p>

<p>$ helm version
Client: &amp;version.Version{SemVer:&ldquo;v2.9.1&rdquo;, GitCommit:&ldquo;20adb27c7c5868466912eebdf6664e7390ebe710&rdquo;, GitTreeState:&ldquo;clean&rdquo;}
Server: &amp;version.Version{SemVer:&ldquo;v2.9.1&rdquo;, GitCommit:&ldquo;20adb27c7c5868466912eebdf6664e7390ebe710&rdquo;, GitTreeState:&ldquo;clean&rdquo;}</p>

<h4 id="测试helm功能">测试Helm功能</h4>

<p>这边部署简单Jenkins来进行功能测试：</p>

<p>$ helm install &ndash;name demo &ndash;set Persistence.Enabled=false stable/jenkins
$ kubectl get po,svc  -l app=demo-jenkins
NAME                           READY     STATUS    RESTARTS   AGE
demo-jenkins-7bf4bfcff-q74nt   <sup>1</sup>&frasl;<sub>1</sub>       Running   0          2m</p>

<p>NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
demo-jenkins         LoadBalancer   10.103.15.129    <pending>     8080:31161/TCP   2m
demo-jenkins-agent   ClusterIP      10.103.160.126   <none>        50000/TCP        2m</p>

<h1 id="取得-admin-账号的密码">取得 admin 账号的密码</h1>

<p>$ printf $(kubectl get secret &ndash;namespace default demo-jenkins -o jsonpath=&ldquo;{.data.jenkins-admin-password}&rdquo; | base64 &ndash;decode);echo
r6y9FMuF2u</p>

<p>当服务都正常运作时，就可以通过浏览器查看<a href="http://node_ip:31161/">HTTP：// node_ip：31161</a>页面。</p>

<p><img src="https://kairen.github.io/images/kube/helm-jenkins-v1.10.png" alt="" /></p>

<p>测试完成后，就可以通过以下指令来删除 Release：</p>

<p>$ helm ls
NAME    REVISION    UPDATED                     STATUS      CHART             NAMESPACE
demo    1           Tue Apr 10 07:29:51 2018    DEPLOYED    jenkins-0.14.4    default</p>

<p>$ helm delete demo &ndash;purge
release &ldquo;demo&rdquo; deleted</p>

<p>想要了解更多Helm Apps的话，可以到<a href="https://hub.kubeapps.com/">Kubeapps Hub</a>网站寻找。</p>

<h2 id="测试集群ha功能">测试集群HA功能</h2>

<p>展示进入首先k8s-m1节点，然后关闭该节点：</p>

<p>$ sudo poweroff</p>

<p>接着进入到k8s-m2节点，通过kubectl来检查集群是否能够正常执行：</p>

<h1 id="先检查-etcd-状态-可以发现-etcd-0-因为关机而中断">先检查 etcd 状态，<em>可以发现 etcd-0 因为关机而中断</em></h1>

<p>$ kubectl get cs
NAME                 STATUS      MESSAGE                                                                                                                                          ERROR
scheduler            Healthy     ok
controller-manager   Healthy     ok
etcd-1               Healthy     {&ldquo;health&rdquo;: &ldquo;true&rdquo;}
etcd-2               Healthy     {&ldquo;health&rdquo;: &ldquo;true&rdquo;}
etcd-0               Unhealthy   Get <a href="https://172.22.132.10:2379/health:">https://172.22.132.10:2379/health:</a> net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</p>

<h1 id="测试是否可以建立-pod">测试是否可以建立 Pod</h1>

<p>$ kubectl run nginx &ndash;image nginx &ndash;restart=Never &ndash;port 80
$ kubectl get po
NAME      READY     STATUS    RESTARTS   AGE
nginx     <sup>1</sup>&frasl;<sub>1</sub>       Running   0          22s</p>

    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">去去</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">2018-08-22</span>
  </p>
  
  
</div>

    
    

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/kubernetes/">kubernetes</a>
          
          <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/%E8%BD%ACkubernetes%E6%98%AF%E4%BB%80%E4%B9%88/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">【转】kubernetes是什么</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        
          <a class="next" href="/post/%E8%BD%ACkubernetes-1.8%E6%89%8B%E5%8A%A8%E6%90%AD%E5%BB%BA/">
            <span class="next-text nav-default">【转】kubernetes 1.8手动搭建</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
  <a href="http://zhulingbiezhi.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">去去</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>








</body>
</html>
