<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>【转】安装kubernetes集群 - 去去的blog - A super concise theme for Hugo</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="去去" /><meta name="description" content="安装kubernetes集群" />
<meta name="keywords" content="kubernetes" />







<meta name="generator" content="Hugo 0.46" />


<link rel="canonical" href="http://zhulingbiezhi.github.io/post/%E8%BD%AC%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">







<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="【转】安装kubernetes集群" />
<meta property="og:description" content="安装kubernetes集群" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://zhulingbiezhi.github.io/post/%E8%BD%AC%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4/" />



<meta property="article:published_time" content="2018-08-09T15:11:13&#43;08:00"/>

<meta property="article:modified_time" content="2018-08-09T15:11:13&#43;08:00"/>











<meta itemprop="name" content="【转】安装kubernetes集群">
<meta itemprop="description" content="安装kubernetes集群">


<meta itemprop="datePublished" content="2018-08-09T15:11:13&#43;08:00" />
<meta itemprop="dateModified" content="2018-08-09T15:11:13&#43;08:00" />
<meta itemprop="wordCount" content="7649">



<meta itemprop="keywords" content="kubernetes,分布式," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="【转】安装kubernetes集群"/>
<meta name="twitter:description" content="安装kubernetes集群"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">去去</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">类别</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">去去</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">类别</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">【转】安装kubernetes集群</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-09 </span>
        <div class="post-category">
            
              <a href="/categories/kubernetes/"> kubernetes </a>
            
          </div>
        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li><a href="#软件环境">软件环境：</a></li>
<li><a href="#准备工作">准备工作</a></li>
<li><a href="#创建-ca-证书和秘钥">创建 CA 证书和秘钥</a></li>
<li><a href="#部署etcd">部署etcd</a></li>
<li><a href="#部署-flannel">部署 Flannel</a></li>
<li><a href="#部署-kubectl-工具-创建kubeconfig文件">部署 kubectl 工具，创建kubeconfig文件</a></li>
<li><a href="#部署-master-节点">部署 master 节点</a></li>
<li><a href="#部署-node-节点">部署 Node 节点</a></li>
<li><a href="#安装dns插件">安装DNS插件</a></li>
<li><a href="#部署-dashboard-插件">部署 dashboard 插件</a></li>
<li><a href="#部署-heapster-插件">部署 heapster 插件</a></li>
<li><a href="#参考资料">参考资料</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>手动安装kubernetes集群，通过安装过程可以了解kubernetes的组成部分。<br />
本安装过程全部采用下载二进制安装包的方式安装，k8s镜像直接从谷歌下载，所以要所有节点配置好科学上网，如何配置科学上网就不在这里介绍了。</p>

<p>采用三个节点安装，vmnode1作为主节点，由于节点少主节点也当做node节点使用。</p>

<table>
<thead>
<tr>
<th align="left">节点名称</th>
<th align="right">ip</th>
<th align="center">组件</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">vmnode1</td>
<td align="right">192.168.123.81</td>
<td align="center">etcd,kube-scheduler,kube-controller-manager,kube-apiserver,kube-proxy,kubelet,docker,kubectl</td>
</tr>

<tr>
<td align="left">vmnode2</td>
<td align="right">192.168.123.82</td>
<td align="center">etcd,kube-proxy,kubelet,docker</td>
</tr>

<tr>
<td align="left">vmnode3</td>
<td align="right">192.168.123.83</td>
<td align="center">etcd,kube-proxy,kubelet,docker</td>
</tr>
</tbody>
</table>

<h1 id="软件环境">软件环境：</h1>

<pre><code>CentOS Linux release 7.4.1708 (Core)  
kubernetes1.8.6  
etcd3.2.12  
flanneld0.9.1  
docker17.12.0-ce
</code></pre>

<h1 id="准备工作">准备工作</h1>

<p>所有节点都要执行下面的操作</p>

<p>修改hosts文件</p>

<pre><code>cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
192.168.123.81 vmnode1
192.168.123.82 vmnode2
192.168.123.83 vmnode3
</code></pre>

<p>各节点禁用防火墙：</p>

<pre><code>systemctl stop firewalld
systemctl disable firewalld
</code></pre>

<p>创建/etc/sysctl.d/k8s.conf文件</p>

<pre><code>cat &lt;&lt; EOF &gt; /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
</code></pre>

<p>加载br_netfilter</p>

<pre><code>modprobe br_netfilter
echo &quot;modprobe br_netfilter&quot; &gt;&gt; /etc/rc.local
</code></pre>

<p>配置生效</p>

<pre><code>sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>

<p>禁用SELINUX</p>

<pre><code>setenforce 0

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
</code></pre>

<p>关闭系统的Swap</p>

<pre><code>$ swapoff -a
</code></pre>

<p>修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。</p>

<p>设置iptables策略为 ACCEPT</p>

<pre><code> /sbin/iptables -P FORWARD ACCEPT
 echo  &quot;sleep 60 &amp;&amp; /sbin/iptables -P FORWARD ACCEPT&quot; &gt;&gt; /etc/rc.local
</code></pre>

<p>安装依赖包</p>

<pre><code>yum install -y epel-release
yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget
</code></pre>

<h1 id="创建-ca-证书和秘钥">创建 CA 证书和秘钥</h1>

<p>kubernetes 系统各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件，CA 是自签名的证书，用来签名后续创建的其它 TLS 证书。</p>

<p>以下操作都在 master 节点即 192.168.123.81 上执行，证书只需要创建一次即可，以后在向集群中添加新节点时只要将 /etc/kubernetes/ 目录下的证书拷贝到新节点上即可</p>

<p>安装 CFSSL</p>

<pre><code>wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
chmod +x cfssl_linux-amd64
sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssljson_linux-amd64
sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl-certinfo_linux-amd64
sudo mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo

export PATH=/usr/local/bin:$PATH
</code></pre>

<p>创建 CA 配置文件</p>

<pre><code>mkdir /root/ssl
cd /root/ssl

cat &gt; ca-config.json &lt;&lt; EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
EOF
</code></pre>

<p>ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；<br />
signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；<br />
server auth：表示 client 可以用该 CA 对 server 提供的证书进行验证；<br />
client auth：表示 server 可以用该 CA 对 client 提供的证书进行验证；</p>

<p>创建 CA 证书签名请求：</p>

<pre><code>cat &gt; ca-csr.json &lt;&lt; EOF
{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
EOF
</code></pre>

<p>“CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；<br />
“O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；</p>

<p>生成 CA 证书和私钥：</p>

<pre><code>cfssl gencert -initca ca-csr.json | cfssljson -bare ca
</code></pre>

<p>创建 kubernetes 证书签名请求文件：</p>

<pre><code>cat &gt; kubernetes-csr.json &lt;&lt; EOF
{
   &quot;CN&quot;: &quot;kubernetes&quot;,
    &quot;hosts&quot;: [
      &quot;127.0.0.1&quot;,
      &quot;192.168.123.81&quot;,
      &quot;192.168.123.82&quot;,
      &quot;192.168.123.83&quot;,
      &quot;10.254.0.1&quot;,
      &quot;kubernetes&quot;,
      &quot;kubernetes.default&quot;,
      &quot;kubernetes.default.svc&quot;,
      &quot;kubernetes.default.svc.cluster&quot;,
      &quot;kubernetes.default.svc.cluster.local&quot;
    ],
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
        {
            &quot;C&quot;: &quot;CN&quot;,
            &quot;ST&quot;: &quot;BeiJing&quot;,
            &quot;L&quot;: &quot;BeiJing&quot;,
            &quot;O&quot;: &quot;k8s&quot;,
            &quot;OU&quot;: &quot;System&quot;
        }
    ]
}
EOF
</code></pre>

<p>hosts 中的内容可以为空，即使按照上面的配置，向集群中增加新节点后也不需要重新生成证书。<br />
如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP</p>

<p>生成 kubernetes 证书和私钥</p>

<pre><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
</code></pre>

<p>创建 admin 证书</p>

<pre><code>cat &gt; admin-csr.json &lt;&lt; EOF
{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
EOF
</code></pre>

<p>kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；<br />
kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限；<br />
OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限</p>

<p>生成 admin 证书和私钥</p>

<pre><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin

ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
</code></pre>

<p>创建 kube-proxy 证书</p>

<pre><code>cat &gt; kube-proxy-csr.json &lt;&lt; EOF
{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
EOF
</code></pre>

<p>CN 指定该证书的 User 为 system:kube-proxy；<br />
kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</p>

<p>生成 kube-proxy 客户端证书和私钥</p>

<pre><code>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy

ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
</code></pre>

<p>分发证书<br />
将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器的 /etc/kubernetes/ssl 目录下</p>

<pre><code>mkdir -p /etc/kubernetes/ssl
cp *.pem /etc/kubernetes/ssl

scp *.pem vmnode2:/etc/kubernetes/ssl
scp *.pem vmnode3:/etc/kubernetes/ssl
</code></pre>

<h1 id="部署etcd">部署etcd</h1>

<p>在三个节点都安装etcd，下面的操作需要再三个节点都执行一遍</p>

<p>下载etcd安装包</p>

<pre><code>wget https://github.com/coreos/etcd/releases/download/v3.2.12/etcd-v3.2.12-linux-amd64.tar.gz
tar -xvf etcd-v3.2.12-linux-amd64.tar.gz
sudo mv etcd-v3.2.12-linux-amd64/etcd* /usr/local/bin
</code></pre>

<p>创建工作目录</p>

<pre><code>sudo mkdir -p /var/lib/etcd
</code></pre>

<p>创建systemd unit 文件</p>

<pre><code>cat &gt; etcd.service &lt;&lt; EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/local/bin/etcd \\
  --name vmnode1 \\
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --initial-advertise-peer-urls https://192.168.123.81:2380 \\
  --listen-peer-urls https://192.168.123.81:2380 \\
  --listen-client-urls https://192.168.123.81:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls https://192.168.123.81:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster vmnode1=https://192.168.123.81:2380,vmnode2=https://192.168.123.82:2380,vmnode3=https://192.168.123.83:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>指定 etcd 的工作目录为 /var/lib/etcd，数据目录为 /var/lib/etcd，需在启动服务前创建这个目录，否则启动服务的时候会报错“Failed at step CHDIR spawning /usr/bin/etcd: No such file or directory”；<br />
为了保证通信安全，需要指定 etcd 的公私钥(cert-file和key-file)、Peers 通信的公私钥和 CA 证书(peer-cert-file、peer-key-file、peer-trusted-ca-file)、客户端的CA证书（trusted-ca-file）；<br />
创建 kubernetes.pem 证书时使用的 kubernetes-csr.json 文件的 hosts 字段包含所有 etcd 节点的IP，否则证书校验会出错；<br />
–initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中；</p>

<p>启动 etcd 服务</p>

<pre><code>sudo cp etcd.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable etcd
sudo systemctl start etcd
systemctl status etcd
</code></pre>

<p>最先启动的 etcd 进程会卡住一段时间，等待其它节点上的 etcd 进程加入集群，为正常现象。</p>

<p>验证etcd服务,在任何一个etcd节点执行</p>

<pre><code>[root@vmnode3 ssl]# etcdctl \
   --ca-file=/etc/kubernetes/ssl/ca.pem \
   --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem 
   cluster-health
member 622bf5e620f5d9eb is healthy: got healthy result from https://192.168.123.82:2379
member 9ecf774850f40c83 is healthy: got healthy result from https://192.168.123.83:2379
member a86648ad4117104e is healthy: got healthy result from https://192.168.123.81:2379
cluster is healthy
</code></pre>

<h1 id="部署-flannel">部署 Flannel</h1>

<p>在三个节点都安装Flannel，下面的操作需要再三个节点都执行一遍</p>

<p>下载安装Flannel</p>

<pre><code>wget https://github.com/coreos/flannel/releases/download/v0.9.1/flannel-v0.9.1-linux-amd64.tar.gz
tar -xzvf flannel-v0.9.1-linux-amd64.tar.gz -C flannel
sudo cp flannel/{flanneld,mk-docker-opts.sh} /usr/local/bin
</code></pre>

<p>向 etcd 写入网段信息<br />
这两个命令只需要任意一个节点上执行一次就可以</p>

<pre><code>etcdctl --endpoints=https://192.168.123.81:2379,https://192.168.123.82:2379,https://192.168.123.83:2379 \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  mkdir /kubernetes/network

etcdctl --endpoints=https://192.168.123.81:2379,https://192.168.123.82:2379,https://192.168.123.83:2379 \
  --ca-file=/etc/kubernetes/ssl/ca.pem \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  mk /kubernetes/network/config '{&quot;Network&quot;:&quot;172.30.0.0/16&quot;,&quot;SubnetLen&quot;:24,&quot;Backend&quot;:{&quot;Type&quot;:&quot;vxlan&quot;}}'
</code></pre>

<p>创建systemd unit 文件</p>

<pre><code>cat &gt; flanneld.service &lt;&lt; EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/usr/local/bin/flanneld \\
  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\
  -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\
  -etcd-endpoints=https://192.168.123.81:2379,https://192.168.123.82:2379,https://192.168.123.83:2379 \\
  -etcd-prefix=/kubernetes/network
ExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
</code></pre>

<p>mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网网段信息写入到 /run/flannel/docker 文件中，后续 docker 启动时使用这个文件中参数值设置 docker0 网桥；<br />
flanneld 使用系统缺省路由所在的接口和其它节点通信，对于有多个网络接口的机器（如，内网和公网），可以用 -iface=enpxx 选项值指定通信接口；</p>

<p>启动Flannel</p>

<pre><code>sudo mv flanneld.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable flanneld
sudo systemctl start flanneld
systemctl status flanneld
</code></pre>

<p>检查flannel服务状态</p>

<pre><code>/usr/local/bin/etcdctl \
 --endpoints=https://192.168.123.81:2379,https://192.168.123.82:2379,https://192.168.123.83:2379 \
 --ca-file=/etc/kubernetes/ssl/ca.pem \
 --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
 --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
 ls /kubernetes/network/subnets

/kubernetes/network/subnets/172.30.81.0-24
/kubernetes/network/subnets/172.30.101.0-24
/kubernetes/network/subnets/172.30.37.0-24
</code></pre>

<h1 id="部署-kubectl-工具-创建kubeconfig文件">部署 kubectl 工具，创建kubeconfig文件</h1>

<p>kubectl是kubernetes的集群管理工具，任何节点通过kubetcl都可以管理整个k8s集群。<br />
本文是在master节点部署，部署成功后会生成 /root/.kube/config 文件，kubectl就是通过这个获取 kube-apiserver 地址、证书、用户名等信息，所以这个文件需要保管好。</p>

<p>下载安装包</p>

<pre><code>cd
wget https://dl.k8s.io/v1.8.6/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz

sudo cp kubernetes/client/bin/kube* /usr/local/bin/
chmod a+x /usr/local/bin/kube*
export PATH=/root/local/bin:$PATH
</code></pre>

<p>创建/root/.kube/config</p>

<pre><code># 设置集群参数,--server指定Master节点ip
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.123.81:6443

# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --embed-certs=true \
  --client-key=/etc/kubernetes/ssl/admin-key.pem

# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin

# 设置默认上下文
kubectl config use-context kubernetes
</code></pre>

<p>admin.pem 证书 O 字段值为 system:masters，kube-apiserver 预定义的 RoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 相关 API 的权限</p>

<p>创建bootstrap.kubeconfig<br />
kubelet访问kube-apiserver的时候是通过bootstrap.kubeconfig进行用户验证。</p>

<pre><code>#生成token 变量
export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')

cat &gt; token.csv &lt;&lt;EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;
EOF

mv token.csv /etc/kubernetes/

# 设置集群参数--server为master节点ip
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.123.81:6443 \
  --kubeconfig=bootstrap.kubeconfig

# 设置客户端认证参数
$ kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig

$ # 设置上下文参数
$ kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

$ # 设置默认上下文
$ kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

$ mv bootstrap.kubeconfig /etc/kubernetes/
</code></pre>

<p>创建kube-proxy.kubeconfig</p>

<pre><code># 设置集群参数 --server参数为master ip
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=https://192.168.123.81:6443 \
  --kubeconfig=kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
mv kube-proxy.kubeconfig /etc/kubernetes/
</code></pre>

<p>设置集群参数和客户端认证参数时 –embed-certs 都为 true，这会将 certificate-authority、client-certificate 和 client-key 指向的证书文件内容写入到生成的 kube-proxy.kubeconfig 文件中；<br />
kube-proxy.pem 证书中 CN 为 system:kube-proxy，kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</p>

<p>生成的bootstrap.kubeconfig，kube-proxy.kubeconfig文件拷贝到其它node节点的/etc/kubernetes目录下</p>

<pre><code>scp /etc/kubernetes/kube-proxy.kubeconfig vmnode2:/etc/kubernetes/
scp /etc/kubernetes/bootstrap.kubeconfig  vmnode2:/etc/kubernetes/

scp /etc/kubernetes/kube-proxy.kubeconfig vmnode3:/etc/kubernetes/
scp /etc/kubernetes/bootstrap.kubeconfig  vmnode3:/etc/kubernetes/
</code></pre>

<h1 id="部署-master-节点">部署 master 节点</h1>

<p>上面的那一堆都是准备工作，下面开始正式部署kubernetes了，<br />
在master节点进行部署。</p>

<p>下载安装文件</p>

<pre><code>wget https://dl.k8s.io/v1.8.6/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
cp -r kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
</code></pre>

<p>配置和启动 kube-apiserver</p>

<pre><code>cat &gt; kube-apiserver.service &lt;&lt; EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --logtostderr=true \\
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\
  --advertise-address=192.168.123.81 \\
  --bind-address=192.168.123.81 \\
  --insecure-bind-address=127.0.0.1 \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\
  --kubelet-https=true \\
  --enable-bootstrap-token-auth \\
  --token-auth-file=/etc/kubernetes/token.csv \\
  --service-cluster-ip-range=10.254.0.0/16 \\
  --service-node-port-range=8400-10000 \\
  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\
  --etcd-servers=https://192.168.123.81:2379,https://192.168.123.82:2379,https://192.168.123.83:2379 \\
  --enable-swagger-ui=true \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/lib/audit.log \\
  --event-ttl=1h \\
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>–authorization-mode=RBAC 指定在安全端口使用 RBAC 授权模式，拒绝未通过授权的请求；<br />
kube-scheduler、kube-controller-manager 一般和 kube-apiserver 部署在同一台机器上，它们使用非安全端口和 kube-apiserver通信;<br />
kubelet、kube-proxy、kubectl 部署在其它 Node 节点上，如果通过安全端口访问 kube-apiserver，则必须先通过 TLS 证书认证，再通过 RBAC 授权；<br />
kube-proxy、kubectl 通过在使用的证书里指定相关的 User、Group 来达到通过 RBAC 授权的目的；<br />
如果使用了 kubelet TLS Boostrap 机制，则不能再指定 –kubelet-certificate-authority、–kubelet-client-certificate 和 –kubelet-client-key 选项，否则后续 kube-apiserver 校验 kubelet 证书时出现 ”x509: certificate signed by unknown authority“ 错误；<br />
–admission-control 值必须包含 ServiceAccount，否则部署集群插件时会失败；<br />
–bind-address 不能为 127.0.0.1；<br />
–runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiVersion<br />
–service-cluster-ip-range 指定 Service Cluster IP 地址段，该地址段不能路由可达；<br />
–service-node-port-range 指定 NodePort 的端口范围；<br />
缺省情况下 kubernetes 对象保存在 etcd /registry 路径下，可以通过 –etcd-prefix 参数进行调整；</p>

<p>启动 kube-apiserver</p>

<pre><code>cp kube-apiserver.service /etc/systemd/system/

systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
</code></pre>

<p>配置和启动 kube-controller-manager</p>

<pre><code>cat &gt; kube-controller-manager.service &lt;&lt; EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --logtostderr=true  \\
  --address=127.0.0.1 \\
  --master=http://127.0.0.1:8080 \\
  --allocate-node-cidrs=true \\
  --service-cluster-ip-range=10.254.0.0/16 \\
  --cluster-cidr=172.30.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --leader-elect=true \\
  --v=2
Restart=on-failure
LimitNOFILE=65536
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器<br />
–master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；<br />
–cluster-cidr 指定 Cluster 中 Pod 的 CIDR 范围，该网段在各 Node 间必须路由可达(flanneld保证)；<br />
–service-cluster-ip-range 参数指定 Cluster 中 Service 的CIDR范围，该网络在各 Node 间必须路由不可达，必须和 kube-apiserver 中的参数一致；<br />
–cluster-signing-* 指定的证书和私钥文件用来签名为 TLS BootStrap 创建的证书和私钥；<br />
–root-ca-file 用来对 kube-apiserver 证书进行校验，指定该参数后，才会在Pod 容器的 ServiceAccount 中放置该 CA 证书文件；<br />
–leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；</p>

<pre><code>cp kube-controller-manager.service /etc/systemd/system/

systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
</code></pre>

<p>配置和启动 kube-scheduler</p>

<pre><code>cat &gt; kube-scheduler.service &lt;&lt; EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --logtostderr=true \\
  --address=127.0.0.1 \\
  --master=http://127.0.0.1:8080 \\
  --leader-elect=true \\
  --v=2
Restart=on-failure
LimitNOFILE=65536
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>–address 值必须为 127.0.0.1，因为当前 kube-apiserver 期望 scheduler 和 controller-manager 在同一台机器；<br />
–master=http://{MASTER_IP}:8080：使用非安全 8080 端口与 kube-apiserver 通信；<br />
–leader-elect=true 部署多台机器组成的 master 集群时选举产生一处于工作状态的 kube-controller-manager 进程；</p>

<p>启动 kube-scheduler</p>

<pre><code>cp kube-scheduler.service /etc/systemd/system/

systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
</code></pre>

<p>验证 master 节点功能</p>

<pre><code>kubectl get componentstatuses
NAME                 STATUS    MESSAGE              ERROR
etcd-1               Healthy   {&quot;health&quot;: &quot;true&quot;}   
etcd-2               Healthy   {&quot;health&quot;: &quot;true&quot;}   
etcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}   
controller-manager   Healthy   ok                   
scheduler            Healthy   ok  
</code></pre>

<h1 id="部署-node-节点">部署 Node 节点</h1>

<p>master节点也作为node节点使用，需要在三个节点都执行安装操作</p>

<p>下载文件</p>

<pre><code>wget https://download.docker.com/linux/static/stable/x86_64/docker-17.12.0-ce.tgz
tar -xvf docker-17.12.0-ce.tgz
cp docker/docker* /usr/local/bin
</code></pre>

<p>配置启动docker</p>

<pre><code>cat &gt; docker.service &lt;&lt; EOF
[Unit]
Description=Docker Application Container Engine
Documentation=http://docs.docker.io

[Service]
Environment=&quot;PATH=/usr/local/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;
EnvironmentFile=-/run/flannel/subnet.env
EnvironmentFile=-/run/flannel/docker
ExecStart=/usr/local/bin/dockerd \\
  --exec-opt native.cgroupdriver=cgroupfs \\
  --log-level=error \\
  --log-driver=json-file \\
  --storage-driver=overlay \\
  \$DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP \$MAINPID
Restart=on-failure
RestartSec=5
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>$DOCKER_NETWORK_OPTIONS和$MAINPID不需要替换；<br />
flanneld 启动时将网络配置写入到 /run/flannel/docker 文件中的变量 DOCKER_NETWORK_OPTIONS，dockerd 命令行上指定该变量值来设置 docker0 网桥参数；<br />
如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；<br />
不能关闭默认开启的 –iptables 和 –ip-masq 选项；<br />
如果内核版本比较新，建议使用 overlay 存储驱动；<br />
–exec-opt native.cgroupdriver=systemd参数可以指定为”cgroupfs”或者“systemd”</p>

<p>启动</p>

<pre><code>cp docker.service /etc/systemd/system/docker.service

systemctl daemon-reload
systemctl enable docker
systemctl start docker
systemctl status docker
</code></pre>

<p>安装和配置 kubelet<br />
kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求</p>

<p>下面这条命令只在master点执行一次即可</p>

<pre><code>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
</code></pre>

<p>下载安装 kubelet 和 kube-proxy</p>

<pre><code>wget https://dl.k8s.io/v1.8.6/kubernetes-server-linux-amd64.tar.gz
tar -xzvf kubernetes-server-linux-amd64.tar.gz
cp -r kubernetes/server/bin/{kube-proxy,kubelet} /usr/local/bin/
</code></pre>

<p>创建kubelet 工作目录</p>

<pre><code>sudo mkdir /var/lib/kubelet
</code></pre>

<p>配置启动kubelet</p>

<pre><code>cat &gt; kubelet.service &lt;&lt; EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \\
  --address=192.168.123.81 \\
  --hostname-override=192.168.123.81 \\
  --pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest \\
  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --require-kubeconfig \\
  --cert-dir=/etc/kubernetes/ssl \\
  --container-runtime=docker \\
  --cluster-dns=10.254.0.2 \\
  --cluster-domain=cluster.local \\
  --hairpin-mode promiscuous-bridge \\
  --allow-privileged=true \\
  --serialize-image-pulls=false \\
  --register-node=true \\
  --logtostderr=true \\
  --cgroup-driver=cgroupfs  \\
  --v=2

Restart=on-failure
KillMode=process
LimitNOFILE=65536
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>–address 是本机ip，不能设置为 127.0.0.1，否则后续 Pods 访问 kubelet 的 API 接口时会失败，因为 Pods 访问的 127.0.0.1 指向自己而不是 kubelet；</p>

<p>–hostname-override 也是本机IP；</p>

<p>–cgroup-driver 配置成 cgroup（保持docker和kubelet中的cgroup driver配置一致即可）;</p>

<p>–experimental-bootstrap-kubeconfig 指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</p>

<p>管理员通过了 CSR 请求后，kubelet 自动在 –cert-dir 目录创建证书和私钥文件(kubelet-client.crt 和 kubelet-client.key)，然后写入 –kubeconfig 文件(自动创建 –kubeconfig 指定的文件)；</p>

<p>建议在 –kubeconfig 配置文件中指定 kube-apiserver 地址，如果未指定 –api-servers 选项，则必须指定 –require-kubeconfig 选项后才从配置文件中读取 kue-apiserver 的地址，否则 kubelet 启动后将找不到 kube-apiserver (日志中提示未找到 API Server），kubectl get nodes 不会返回对应的 Node 信息;</p>

<p>–cluster-dns 指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，–cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；</p>

<p>–cluster-domain 指定 pod 启动时 /etc/resolve.conf 文件中的 search domain ，起初我们将其配置成了 cluster.local.，这样在解析 service 的 DNS 名称时是正常的，可是在解析 headless service 中的 FQDN pod name 的时候却错误，因此我们将其修改为 cluster.local，去掉嘴后面的 ”点号“ 就可以解决该问题；</p>

<p>–kubeconfig=/etc/kubernetes/kubelet.kubeconfig中指定的kubelet.kubeconfig文件在第一次启动kubelet之前并不存在，请看下文，当通过CSR请求后会自动生成kubelet.kubeconfig文件，如果你的节点上已经生成了~/.kube/config文件，你可以将该文件拷贝到该路径下，并重命名为kubelet.kubeconfig，所有node节点可以共用同一个kubelet.kubeconfig文件，这样新添加的节点就不需要再创建CSR请求就能自动添加到kubernetes集群中。同样，在任意能够访问到kubernetes集群的主机上使用kubectl –kubeconfig命令操作集群时，只要使用~/.kube/config文件就可以通过权限认证，因为这里面已经有认证信息并认为你是admin用户，对集群拥有所有权限。</p>

<p>启动 kubelet</p>

<pre><code>cp kubelet.service /etc/systemd/system/kubelet.service

systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
</code></pre>

<p>执行TLS 证书授权请求<br />
kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须授权通过后，Node才会加入到集群中<br />
在三个节点都部署完kubelet之后，在master节点执行授权操作</p>

<pre><code>#查询授权请求
[root@vmnode1 ~]# kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-2wlNlozltgRg9l_V2ahByL8bwRWjMOwMqCivvGssDWE   4m        kubelet-bootstrap   Pending
node-csr-hTaFwUYEHAelgOD2HfO5t2OSaw8oWW5cq_MdZ_yOWeA   1m        kubelet-bootstrap   Pending
node-csr-lzxU0UE7TU7jtVXvLmPMq_giRfhcBo-azmHMLXZalXo   4m        kubelet-bootstrap   Pending

#授权：
[root@vmnode1 ~]# kubectl certificate approve node-csr-2wlNlozltgRg9l_V2ahByL8bwRWjMOwMqCivvGssDWE
certificatesigningrequest &quot;node-csr-2wlNlozltgRg9l_V2ahByL8bwRWjMOwMqCivvGssDWE&quot; approved

[root@vmnode1 ~]# kubectl certificate approve node-csr-hTaFwUYEHAelgOD2HfO5t2OSaw8oWW5cq_MdZ_yOWeA
certificatesigningrequest &quot;node-csr-hTaFwUYEHAelgOD2HfO5t2OSaw8oWW5cq_MdZ_yOWeA&quot; approved

[root@vmnode1 ~]# kubectl certificate approve node-csr-lzxU0UE7TU7jtVXvLmPMq_giRfhcBo-azmHMLXZalXo
certificatesigningrequest &quot;node-csr-lzxU0UE7TU7jtVXvLmPMq_giRfhcBo-azmHMLXZalXo&quot; approved

#查看已加入集群的节点
[root@vmnode1 ~]# kubectl get nodes
NAME             STATUS    ROLES     AGE       VERSION
192.168.123.81   Ready     &lt;none&gt;    23h       v1.8.6
192.168.123.82   Ready     &lt;none&gt;    23h       v1.8.6
192.168.123.83   Ready     &lt;none&gt;    23h       v1.8.6
</code></pre>

<p>创建kube-proxy工作目录</p>

<pre><code>sudo mkdir -p /var/lib/kube-proxy
</code></pre>

<p>配置启动kube-proxy</p>

<pre><code>cat &gt; kube-proxy.service &lt;&lt; EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \\
  --bind-address=192.168.123.81 \\
  --hostname-override=192.168.123.81 \\
  --cluster-cidr=10.254.0.0/16 \\
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<p>–bind-address 参数为本机IP<br />
–hostname-override 参数为本机IP，值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则；</p>

<p>–cluster-cidr 必须与 kube-apiserver 的 –service-cluster-ip-range 选项值一致，kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</p>

<p>–kubeconfig 指定的配置文件嵌入了 kube-apiserver 的地址、用户名、证书、秘钥等请求和认证信息；</p>

<p>预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</p>

<p>启动 kube-proxy</p>

<pre><code>cp kube-proxy.service /etc/systemd/system/

systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
</code></pre>

<p>在另外的两个节点进行上面的部署操作，注意替换其中的ip。</p>

<h1 id="安装dns插件">安装DNS插件</h1>

<p>在主节点上进行安装操作</p>

<p>下载安装文件</p>

<pre><code>wget https://github.com/kubernetes/kubernetes/releases/download/v1.8.6/kubernetes.tar.gz
tar xzvf kubernetes.tar.gz

cd /root/kubernetes/cluster/addons/dns
mv  kubedns-svc.yaml.sed kubedns-svc.yaml
#把文件中$DNS_SERVER_IP替换成10.254.0.2
sed -i 's/$DNS_SERVER_IP/10.254.0.2/g' ./kubedns-svc.yaml

mv ./kubedns-controller.yaml.sed ./kubedns-controller.yaml
#把$DNS_DOMAIN替换成cluster.local
sed -i 's/$DNS_DOMAIN/cluster.local/g' ./kubedns-controller.yaml

ls *.yaml
kubedns-cm.yaml  kubedns-controller.yaml  kubedns-sa.yaml  kubedns-svc.yaml

kubectl create -f .
</code></pre>

<h1 id="部署-dashboard-插件">部署 dashboard 插件</h1>

<p>在master节点部署</p>

<p>下载部署文件</p>

<pre><code>wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.8.1/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre>

<p>修改kubernetes-dashboard.yaml ##定位到kind: Service部分，增加type: NodePort</p>

<pre><code>kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 8510
  selector:
    k8s-app: kubernetes-dashboard
</code></pre>

<p>创建kubernetes-dashboard</p>

<pre><code>kubectl create -f kubernetes-dashboard.yaml
</code></pre>

<blockquote>
<p>如果dashboard出现<br />
configmaps is forbidden: User “system:serviceaccount:kube-system:kubernetes-dashboard” cannot list configmaps in the namespace “default” 错误，需要对ServiceAccount授权</p>
</blockquote>

<p>创建一个kubernetes-dashboard-admin.rbac.yaml文件</p>

<pre><code>cat &gt; ./kubernetes-dashboard-admin.rbac.yaml &lt;&lt; EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
EOF

kubectl create -f kubernetes-dashboard-admin.rbac.yaml 
</code></pre>

<p>dashboard登录地址</p>

<p><a href="https://192.168.123.81:8510">https://192.168.123.81:8510</a></p>

<h1 id="部署-heapster-插件">部署 heapster 插件</h1>

<p>下载安装文件</p>

<pre><code>wget https://github.com/kubernetes/heapster/archive/v1.5.0.tar.gz
tar xzvf ./v1.5.0.tar.gz
cd ./heapster-1.5.0/

kubectl create -f deploy/kube-config/influxdb/
kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml
</code></pre>

<p>确认所有pod都正常启动</p>

<pre><code>kubectl get pods --all-namespaces
</code></pre>

<p>登录dashboard<br />
<a href="https://192.168.123.81:8510">https://192.168.123.81:8510</a></p>

<p><img src="https://img-blog.csdn.net/20180104111209127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbmV3Y3JhbmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" /></p>

<h1 id="参考资料">参考资料</h1>

<p><a href="https://k8s-install.opsnull.com/">https://k8s-install.opsnull.com/</a><br />
<a href="https://jimmysong.io/kubernetes-handbook/practice/install-kubernetes-on-centos.html">https://jimmysong.io/kubernetes-handbook/practice/install-kubernetes-on-centos.html</a></p>

    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">去去</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">2018-08-09</span>
  </p>
  
  
</div>

    
    

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/kubernetes/">kubernetes</a>
          
          <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/post/%E8%BD%ACtcp%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E4%B8%8E%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/">
            <span class="next-text nav-default">【转】浅谈TCP流量控制与拥塞控制</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:your@email.com" class="iconfont icon-email" title="email"></a>
      <a href="http://localhost:1313" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="http://localhost:1313" class="iconfont icon-twitter" title="twitter"></a>
      <a href="http://localhost:1313" class="iconfont icon-facebook" title="facebook"></a>
      <a href="http://localhost:1313" class="iconfont icon-linkedin" title="linkedin"></a>
      <a href="http://localhost:1313" class="iconfont icon-google" title="google"></a>
      <a href="http://localhost:1313" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="http://localhost:1313" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="http://localhost:1313" class="iconfont icon-douban" title="douban"></a>
      <a href="http://localhost:1313" class="iconfont icon-pocket" title="pocket"></a>
      <a href="http://localhost:1313" class="iconfont icon-tumblr" title="tumblr"></a>
      <a href="http://localhost:1313" class="iconfont icon-instagram" title="instagram"></a>
  <a href="http://zhulingbiezhi.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">去去</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>








</body>
</html>
